{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "364bd179",
      "metadata": {
        "id": "364bd179"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the CSV file\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\heart_disease_uci.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Columns with missing values\n",
        "columns_with_missing_values = ['trestbps', 'chol', 'thalch', 'oldpeak','fbs', 'restecg', 'ca', 'slope', 'thal']\n",
        "\n",
        "# Drop missing values from specified columns\n",
        "df.dropna(subset=columns_with_missing_values, inplace=True)\n",
        "\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\missing.csv'\n",
        "df.to_csv(new_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c25db6c",
      "metadata": {
        "id": "7c25db6c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\missing.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "label_encoder = LabelEncoder()\n",
        "columns=['sex','cp','fbs','restecg','exang','slope','thal']\n",
        "for column in columns:\n",
        "    df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\encoded.csv'\n",
        "df.to_csv(new_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea177aa",
      "metadata": {
        "id": "9ea177aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\encoded.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "columns_tonormalize = ['age','cp','trestbps', 'chol', 'restecg', 'thalch', 'oldpeak', 'slope', 'ca', 'thal']\n",
        "scaler = MinMaxScaler()\n",
        "df[columns_tonormalize] = scaler.fit_transform(df[columns_tonormalize])\n",
        "\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\normalized.csv'\n",
        "df.to_csv(new_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629182e0",
      "metadata": {
        "id": "629182e0"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "import pandas as pd\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\normalized.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "outliers = isolation_forest.fit_predict(df)\n",
        "\n",
        "df_filtered = df[outliers != -1]\n",
        "\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\outlier.csv'\n",
        "df_filtered.to_csv(new_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5450f1",
      "metadata": {
        "id": "fd5450f1",
        "outputId": "78189f4d-8580-4940-c06b-45c92f39bab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.24.3)\n",
            "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.11.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.1.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc37e505",
      "metadata": {
        "id": "dc37e505"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\outlier.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "X = df.drop('num', axis=1)\n",
        "y = df['num']\n",
        "\n",
        "\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "resampled_df['num'] = y_resampled\n",
        "resampled_df = resampled_df.sample(frac=1)\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smote.csv'\n",
        "resampled_df.to_csv(new_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed3896f7",
      "metadata": {
        "id": "ed3896f7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab23101",
      "metadata": {
        "id": "dab23101"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "import pandas as pd\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smote.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "outliers = isolation_forest.fit_predict(df)\n",
        "\n",
        "df_filtered = df[outliers != -1]\n",
        "\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\outlier.csv'\n",
        "df_filtered.to_csv(new_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41271f8",
      "metadata": {
        "id": "a41271f8",
        "outputId": "6406c3b2-c3fb-4750-d9d4-03ff7ede63cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target correlation\n",
            "[('chol', -0.013593902300911086), ('thalch', -0.021985098843714235), ('age', -0.04193520158417837), ('oldpeak', -0.05294047079278687), ('trestbps', -0.053183258567897254), ('ca', -0.21501819608645567), ('thal', -0.4901766309415492), ('cp', -0.5767915480527931), ('restecg', -0.6593061012316944), ('slope', -0.6795691295407513), ('exang', -1.2827238980468072), ('sex', -1.735960382385049), ('fbs', -1.9835528936757367)]\n",
            "Overlapping of features\n",
            "[('chol', 'fbs', -0.0006249783293560339), ('thalch', 'fbs', -0.00103305780782925), ('age', 'fbs', -0.001050643189835077), ('trestbps', 'fbs', -0.0012955393460958053), ('oldpeak', 'fbs', -0.0016603698369147498), ('chol', 'sex', -0.0022034778988785134), ('ca', 'fbs', -0.004293183637767436), ('thalch', 'sex', -0.005484189219011797), ('age', 'sex', -0.006693012022777328), ('thal', 'fbs', -0.007041948734600569), ('chol', 'thal', -0.00896177533245242), ('chol', 'restecg', -0.009180538900846729), ('chol', 'exang', -0.009489248362598532), ('cp', 'fbs', -0.009642223841525892), ('chol', 'slope', -0.01004616860750491), ('trestbps', 'sex', -0.010062684562831119), ('restecg', 'fbs', -0.010413041777448813), ('chol', 'ca', -0.010659362145214362), ('chol', 'cp', -0.011507588802935865), ('thalch', 'exang', -0.01228172788697058), ('oldpeak', 'slope', -0.012328499240325305), ('slope', 'fbs', -0.012378408456372168), ('thalch', 'slope', -0.012865147975253661), ('oldpeak', 'sex', -0.01351707726420152), ('chol', 'trestbps', -0.013858166931135285), ('chol', 'oldpeak', -0.014410300833377607), ('chol', 'thalch', -0.014519543734882167), ('chol', 'age', -0.014959721067124239), ('thalch', 'cp', -0.015807914462678544), ('thalch', 'ca', -0.017587150819936094), ('thalch', 'restecg', -0.017602800915402134), ('thalch', 'thal', -0.024071166919473752), ('age', 'thal', -0.024802768110807796), ('age', 'restecg', -0.025723094330031407), ('age', 'slope', -0.02625496831402502), ('age', 'ca', -0.027936835098135767), ('exang', 'fbs', -0.028047433735576086), ('oldpeak', 'exang', -0.02874679493748934), ('age', 'exang', -0.029207512691450173), ('thalch', 'oldpeak', -0.030261939403217097), ('age', 'cp', -0.03130969799350092), ('trestbps', 'thal', -0.03176587353532131), ('thalch', 'chol', -0.03310948619991264), ('thalch', 'trestbps', -0.033155104823622404), ('trestbps', 'exang', -0.03369802476808179), ('trestbps', 'slope', -0.03370457567600535), ('thalch', 'age', -0.034024257463630324), ('trestbps', 'restecg', -0.0341358919852453), ('oldpeak', 'ca', -0.03597273261898818), ('trestbps', 'ca', -0.037922366700036535), ('trestbps', 'cp', -0.04048162968804275), ('age', 'oldpeak', -0.04282337227529927), ('oldpeak', 'thal', -0.04656390691355804), ('age', 'trestbps', -0.04709976123902423), ('oldpeak', 'restecg', -0.04824288406675478), ('age', 'chol', -0.05045535831574296), ('age', 'thalch', -0.05093274156313602), ('ca', 'sex', -0.05201057501007324), ('oldpeak', 'cp', -0.05947270693168226), ('trestbps', 'oldpeak', -0.06368961042310574), ('trestbps', 'age', -0.0728076850096511), ('trestbps', 'chol', -0.07504776877971928), ('trestbps', 'thalch', -0.07576688205040151), ('oldpeak', 'trestbps', -0.09710130502390123), ('thal', 'sex', -0.10023509383442822), ('oldpeak', 'age', -0.10193029824268919), ('oldpeak', 'thalch', -0.106258361602194), ('oldpeak', 'chol', -0.10896327011867153), ('sex', 'fbs', -0.10931198646392061), ('cp', 'sex', -0.12177859224073087), ('ca', 'exang', -0.12993350433805217), ('restecg', 'sex', -0.15829682497468023), ('slope', 'sex', -0.17296794934335702), ('ca', 'slope', -0.18982321805153296), ('ca', 'restecg', -0.22870311305704266), ('ca', 'cp', -0.2597254366558486), ('ca', 'thal', -0.28961146564201823), ('thal', 'exang', -0.29676002949431696), ('cp', 'exang', -0.3129631952915026), ('ca', 'oldpeak', -0.4149824165577241), ('exang', 'sex', -0.41678118157810634), ('restecg', 'exang', -0.42118518000318844), ('ca', 'trestbps', -0.44904220603799444), ('slope', 'exang', -0.4524970638846342), ('ca', 'age', -0.4636134276466485), ('ca', 'thalch', -0.46934025289606074), ('ca', 'chol', -0.4800169828313659), ('thal', 'slope', -0.5571731236090051), ('thal', 'cp', -0.5601332852081407), ('restecg', 'slope', -0.5944449116874065), ('cp', 'slope', -0.6264240292779224), ('thal', 'restecg', -0.6421329315707702), ('cp', 'restecg', -0.6955402808535172), ('cp', 'thal', -0.7008704878459184), ('slope', 'restecg', -0.743754982101294), ('thal', 'ca', -0.7693550716274338), ('restecg', 'cp', -0.8054836507288247), ('thal', 'oldpeak', -0.8679782723566765), ('cp', 'ca', -0.8699067248392477), ('thal', 'trestbps', -0.880766758957879), ('slope', 'cp', -0.8846178645109735), ('restecg', 'thal', -0.8918475379212665), ('thal', 'age', -0.8947646195774507), ('thal', 'chol', -0.9074827160128532), ('thal', 'thalch', -0.9076920449961591), ('restecg', 'ca', -0.9369495249472716), ('slope', 'thal', -0.9525127502229063), ('cp', 'oldpeak', -0.9945983127430853), ('cp', 'trestbps', -1.001512288116323), ('cp', 'thalch', -1.0092793818869803), ('cp', 'age', -1.0119910764284876), ('cp', 'chol', -1.0189176410563192), ('slope', 'ca', -1.0252923361037787), ('restecg', 'oldpeak', -1.0671238525110223), ('restecg', 'trestbps', -1.0775484516452596), ('restecg', 'age', -1.0877218221124425), ('restecg', 'thalch', -1.091688231542645), ('restecg', 'chol', -1.0964285908753635), ('slope', 'oldpeak', -1.1377183757529221), ('slope', 'trestbps', -1.1817145138862142), ('slope', 'thalch', -1.1891018578647399), ('slope', 'age', -1.1913762879060286), ('slope', 'chol', -1.1983767604570532), ('exang', 'slope', -1.2814391344098734), ('exang', 'cp', -1.3448841742695914), ('exang', 'restecg', -1.3700763970489593), ('exang', 'thal', -1.428333693128164), ('sex', 'exang', -1.5483796451677188), ('exang', 'ca', -1.5487009016527382), ('exang', 'oldpeak', -1.6134849217804907), ('exang', 'thalch', -1.6244753721807828), ('exang', 'chol', -1.6281448206718256), ('exang', 'trestbps', -1.6307079123906858), ('exang', 'age', -1.635439040599863), ('sex', 'slope', -1.7610836177236298), ('sex', 'cp', -1.7964199127441085), ('sex', 'restecg', -1.7984911618659507), ('sex', 'thal', -1.8146013020964336), ('sex', 'ca', -1.8719211857775413), ('sex', 'chol', -1.8886304562879592), ('sex', 'oldpeak', -1.889023319445945), ('sex', 'age', -1.889136421828054), ('fbs', 'sex', -1.8892959177340838), ('sex', 'trestbps', -1.8895368533569812), ('sex', 'thalch', -1.8898416550928043), ('fbs', 'exang', -1.9645106601751323), ('fbs', 'slope', -1.985829834891108), ('fbs', 'restecg', -1.9875319654906627), ('fbs', 'cp', -1.989133461990001), ('fbs', 'thal', -1.9892177612221758), ('fbs', 'ca', -1.9933468356774613), ('fbs', 'chol', -1.993832769583354), ('fbs', 'age', -1.9940134162861505), ('fbs', 'trestbps', -1.9940753750336158), ('fbs', 'thalch', -1.9941136181077959), ('fbs', 'oldpeak', -1.994195326907544)]\n",
            "Columns selected by SU feature selection method:\n",
            "['age', 'sex', 'cp', 'trestbps', 'chol', 'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from math import log2\n",
        "\n",
        "def calculate_entropy(data):\n",
        "\n",
        "    value_counts = data.value_counts(normalize=True)\n",
        "    entropy = -sum(p * log2(p) for p in value_counts)\n",
        "    return entropy\n",
        "\n",
        "def calculate_joint_entropy(x, y):\n",
        "\n",
        "    joint_data = pd.concat([x, y], axis=1)\n",
        "    joint_entropy = calculate_entropy(joint_data)\n",
        "    return joint_entropy\n",
        "\n",
        "def calculate_su(x, y):\n",
        "\n",
        "    entropy_x = calculate_entropy(x)\n",
        "    entropy_y = calculate_entropy(y)\n",
        "    joint_entropy_xy = calculate_joint_entropy(x, y)\n",
        "    su = 2 * (entropy_x - joint_entropy_xy) / (entropy_x + entropy_y)\n",
        "    return su\n",
        "\n",
        "def feature_selection_by_su(data, target_column, threshold=0.7):\n",
        "\n",
        "    su_values = {} #  su between feature and target\n",
        "    for column in data.columns:\n",
        "        if column != target_column:\n",
        "            su_values[column] = calculate_su(data[column], data[target_column])\n",
        "\n",
        "    selected_columns = []\n",
        "#    to remove less correlated features\n",
        "    sorted_su_values = sorted(su_values.items(), key=lambda x: x[1], reverse=True)\n",
        "    last_item_su_value = sorted_su_values[-1][1]\n",
        "    if -0.02 <= last_item_su_value <= 0:\n",
        "        column_to_drop = sorted_su_values[-1][0]\n",
        "        data.drop(columns=[column_to_drop], inplace=True)\n",
        "\n",
        "#    su between feature to feature\n",
        "    su_values = {}\n",
        "    for column in data.columns:\n",
        "        if column != target_column:\n",
        "            su_values[column] = []\n",
        "            for other_column in data.columns:\n",
        "                if other_column != column and other_column != target_column:\n",
        "                    su_value = calculate_su(data[column], data[other_column])\n",
        "                    su_values[column].append((other_column, su_value))\n",
        "\n",
        "    print('Target correlation')\n",
        "    print(sorted_su_values)\n",
        "#    for redundant features removal\n",
        "    tosort = []\n",
        "    highly_correlated_features = []\n",
        "\n",
        "    for feature, correlations in su_values.items():\n",
        "        for correlation in correlations:\n",
        "            other_feature, su_value = correlation\n",
        "            tosort.append((feature, other_feature, su_value))\n",
        "\n",
        "#     unique_tosort = set(tosort)\n",
        "#     print(len(unique_tosort))\n",
        "    sorted_unique_tosort = sorted(tosort, key=lambda x: abs(x[2]))\n",
        "    print('Overlapping of features')\n",
        "    print(sorted_unique_tosort)\n",
        "\n",
        "    index_map = {feature: i for i, (feature, _) in enumerate(sorted_su_values)}\n",
        "\n",
        "    features_to_remove = []\n",
        "\n",
        "    for i in range(-3, 0):\n",
        "        f1, f2, _ = sorted_unique_tosort[i]\n",
        "        if f1 in [item[0] for item in sorted_su_values] and f2 in [item[0] for item in sorted_su_values]:\n",
        "            f1_index = index_map[f1]\n",
        "            f2_index = index_map[f2]\n",
        "            feature_to_remove = f1 if f1_index > f2_index else f2\n",
        "#             print(feature_to_remove)\n",
        "            features_to_remove.append(feature_to_remove)\n",
        "\n",
        "\n",
        "    if features_to_remove:\n",
        "        max_index = max(index_map[feature] for feature in features_to_remove)\n",
        "        feature_to_remove = next(feature for feature, index in index_map.items() if index == max_index)\n",
        "        data.drop(columns=[feature_to_remove], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "    selected_columns.extend(data.columns)\n",
        "    return data, selected_columns\n",
        "\n",
        "\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smoteagain.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "target_column = 'num'\n",
        "# print(data.columns)\n",
        "updated_data, selected_columns = feature_selection_by_su(data, target_column)\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv'\n",
        "updated_data.to_csv(new_file_path, index=False)\n",
        "\n",
        "print(\"Columns selected by SU feature selection method:\")\n",
        "print(selected_columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f26ee062",
      "metadata": {
        "id": "f26ee062",
        "outputId": "3d3488e7-5b7e-4298-990e-6676b6007935"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9057971014492754\n",
            "Recall: 0.9057971014492754\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\outlier.csv'\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Assume 'target_column' is the column you want to predict\n",
        "target_column = \"num\"\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "# data.drop(columns=['trestbps'], inplace=True)\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate recall\n",
        "# Choose one of the average options: 'micro', 'macro', or 'weighted'\n",
        "recall = recall_score(y_test, y_pred, average='micro')\n",
        "print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de62272f",
      "metadata": {
        "id": "de62272f",
        "outputId": "7ede4ba2-e6f7-4958-9880-6112a6595389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.927536231884058\n",
            "Recall: 0.927536231884058\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the target column\n",
        "target_column = \"num\"\n",
        "# data.drop(columns=['age'], inplace=True)\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Extra Trees Classifier\n",
        "et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "et_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = et_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate recall\n",
        "# Choose one of the average options: 'micro', 'macro', or 'weighted'\n",
        "recall = recall_score(y_test, y_pred, average='micro')\n",
        "print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69608e26",
      "metadata": {
        "id": "69608e26",
        "outputId": "6d083b5a-17f3-4523-cb56-7aad5e7253c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting lightgbm\n",
            "  Obtaining dependency information for lightgbm from https://files.pythonhosted.org/packages/e1/4c/4685ccfae9806f561de716e32549190c1f533dde5bcadaf83bdf23972cf0/lightgbm-4.3.0-py3-none-win_amd64.whl.metadata\n",
            "  Downloading lightgbm-4.3.0-py3-none-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from lightgbm) (1.24.3)\n",
            "Requirement already satisfied: scipy in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from lightgbm) (1.11.1)\n",
            "Downloading lightgbm-4.3.0-py3-none-win_amd64.whl (1.3 MB)\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/1.3 MB 325.1 kB/s eta 0:00:05\n",
            "    --------------------------------------- 0.0/1.3 MB 325.1 kB/s eta 0:00:05\n",
            "   - -------------------------------------- 0.0/1.3 MB 195.7 kB/s eta 0:00:07\n",
            "   - -------------------------------------- 0.0/1.3 MB 195.7 kB/s eta 0:00:07\n",
            "   - -------------------------------------- 0.0/1.3 MB 195.7 kB/s eta 0:00:07\n",
            "   - -------------------------------------- 0.0/1.3 MB 195.7 kB/s eta 0:00:07\n",
            "   - -------------------------------------- 0.0/1.3 MB 195.7 kB/s eta 0:00:07\n",
            "   - -------------------------------------- 0.0/1.3 MB 195.7 kB/s eta 0:00:07\n",
            "   - -------------------------------------- 0.0/1.3 MB 195.7 kB/s eta 0:00:07\n",
            "   --- ------------------------------------ 0.1/1.3 MB 225.3 kB/s eta 0:00:06\n",
            "   ---- ----------------------------------- 0.1/1.3 MB 243.4 kB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 0.1/1.3 MB 243.4 kB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 0.1/1.3 MB 243.4 kB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 0.1/1.3 MB 243.4 kB/s eta 0:00:05\n",
            "   ---- ----------------------------------- 0.2/1.3 MB 213.6 kB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 0.2/1.3 MB 205.6 kB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 0.2/1.3 MB 226.7 kB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 0.2/1.3 MB 226.7 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 211.0 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 211.0 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 211.0 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------ --------------------------------- 0.2/1.3 MB 199.3 kB/s eta 0:00:06\n",
            "   ------- -------------------------------- 0.3/1.3 MB 152.6 kB/s eta 0:00:08\n",
            "   ------- -------------------------------- 0.3/1.3 MB 152.6 kB/s eta 0:00:08\n",
            "   ------- -------------------------------- 0.3/1.3 MB 152.6 kB/s eta 0:00:08\n",
            "   --------- ------------------------------ 0.3/1.3 MB 168.2 kB/s eta 0:00:07\n",
            "   --------- ------------------------------ 0.3/1.3 MB 168.2 kB/s eta 0:00:07\n",
            "   --------- ------------------------------ 0.3/1.3 MB 168.2 kB/s eta 0:00:07\n",
            "   --------- ------------------------------ 0.3/1.3 MB 169.3 kB/s eta 0:00:06\n",
            "   --------- ------------------------------ 0.3/1.3 MB 169.3 kB/s eta 0:00:06\n",
            "   ---------- ----------------------------- 0.3/1.3 MB 166.4 kB/s eta 0:00:07\n",
            "   ---------- ----------------------------- 0.4/1.3 MB 171.3 kB/s eta 0:00:06\n",
            "   ----------- ---------------------------- 0.4/1.3 MB 171.2 kB/s eta 0:00:06\n",
            "   ----------- ---------------------------- 0.4/1.3 MB 177.0 kB/s eta 0:00:06\n",
            "   ----------- ---------------------------- 0.4/1.3 MB 177.0 kB/s eta 0:00:06\n",
            "   ------------ --------------------------- 0.4/1.3 MB 184.6 kB/s eta 0:00:05\n",
            "   ------------- -------------------------- 0.5/1.3 MB 193.0 kB/s eta 0:00:05\n",
            "   -------------- ------------------------- 0.5/1.3 MB 206.6 kB/s eta 0:00:05\n",
            "   --------------- ------------------------ 0.5/1.3 MB 216.9 kB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 0.6/1.3 MB 225.5 kB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 0.6/1.3 MB 231.0 kB/s eta 0:00:04\n",
            "   ------------------ --------------------- 0.6/1.3 MB 236.1 kB/s eta 0:00:04\n",
            "   ------------------ --------------------- 0.6/1.3 MB 235.7 kB/s eta 0:00:04\n",
            "   ------------------ --------------------- 0.6/1.3 MB 240.8 kB/s eta 0:00:03\n",
            "   ------------------ --------------------- 0.6/1.3 MB 240.8 kB/s eta 0:00:03\n",
            "   ------------------- -------------------- 0.7/1.3 MB 237.3 kB/s eta 0:00:03\n",
            "   ------------------- -------------------- 0.7/1.3 MB 238.3 kB/s eta 0:00:03\n",
            "   ------------------- -------------------- 0.7/1.3 MB 238.3 kB/s eta 0:00:03\n",
            "   -------------------- ------------------- 0.7/1.3 MB 237.6 kB/s eta 0:00:03\n",
            "   -------------------- ------------------- 0.7/1.3 MB 237.6 kB/s eta 0:00:03\n",
            "   -------------------- ------------------- 0.7/1.3 MB 234.8 kB/s eta 0:00:03\n",
            "   --------------------- ------------------ 0.7/1.3 MB 236.7 kB/s eta 0:00:03\n",
            "   --------------------- ------------------ 0.7/1.3 MB 236.7 kB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 0.7/1.3 MB 235.0 kB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 0.7/1.3 MB 234.7 kB/s eta 0:00:03\n",
            "   ---------------------- ----------------- 0.8/1.3 MB 238.9 kB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 0.8/1.3 MB 239.7 kB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 0.8/1.3 MB 241.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------ --------------- 0.8/1.3 MB 245.4 kB/s eta 0:00:03\n",
            "   ------------------------- -------------- 0.8/1.3 MB 204.3 kB/s eta 0:00:03\n",
            "   -------------------------- ------------- 0.9/1.3 MB 206.2 kB/s eta 0:00:03\n",
            "   -------------------------- ------------- 0.9/1.3 MB 206.2 kB/s eta 0:00:03\n",
            "   -------------------------- ------------- 0.9/1.3 MB 203.3 kB/s eta 0:00:03\n",
            "   -------------------------- ------------- 0.9/1.3 MB 203.3 kB/s eta 0:00:03\n",
            "   --------------------------- ------------ 0.9/1.3 MB 208.3 kB/s eta 0:00:02\n",
            "   ---------------------------- ----------- 1.0/1.3 MB 213.0 kB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 1.0/1.3 MB 217.7 kB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 1.0/1.3 MB 217.7 kB/s eta 0:00:02\n",
            "   ------------------------------ --------- 1.0/1.3 MB 220.7 kB/s eta 0:00:02\n",
            "   ------------------------------ --------- 1.0/1.3 MB 223.6 kB/s eta 0:00:02\n",
            "   ------------------------------ --------- 1.0/1.3 MB 223.6 kB/s eta 0:00:02\n",
            "   ------------------------------- -------- 1.1/1.3 MB 224.2 kB/s eta 0:00:02\n",
            "   -------------------------------- ------- 1.1/1.3 MB 226.4 kB/s eta 0:00:02\n",
            "   -------------------------------- ------- 1.1/1.3 MB 226.3 kB/s eta 0:00:02\n",
            "   --------------------------------- ------ 1.1/1.3 MB 228.3 kB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.1/1.3 MB 232.4 kB/s eta 0:00:01\n",
            "   ---------------------------------- ----- 1.2/1.3 MB 235.1 kB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 1.2/1.3 MB 239.1 kB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 1.2/1.3 MB 239.1 kB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.2/1.3 MB 236.6 kB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.2/1.3 MB 236.6 kB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.2/1.3 MB 235.6 kB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.2/1.3 MB 235.6 kB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.2/1.3 MB 235.6 kB/s eta 0:00:01\n",
            "   ------------------------------------- -- 1.2/1.3 MB 233.9 kB/s eta 0:00:01\n",
            "   ------------------------------------- -- 1.3/1.3 MB 234.4 kB/s eta 0:00:01\n",
            "   -------------------------------------- - 1.3/1.3 MB 235.5 kB/s eta 0:00:01\n",
            "   -------------------------------------- - 1.3/1.3 MB 235.4 kB/s eta 0:00:01\n",
            "   ---------------------------------------  1.3/1.3 MB 235.8 kB/s eta 0:00:01\n",
            "   ---------------------------------------  1.3/1.3 MB 235.8 kB/s eta 0:00:01\n",
            "   ---------------------------------------  1.3/1.3 MB 236.8 kB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.3/1.3 MB 236.0 kB/s eta 0:00:00\n",
            "Installing collected packages: lightgbm\n",
            "Successfully installed lightgbm-4.3.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install lightgbm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6936850e",
      "metadata": {
        "id": "6936850e",
        "outputId": "62f20cf9-e01e-484e-e905-d1dc3f831a21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy (RBF Kernel):  49.28\n",
            "F1 (RBF Kernel):  49.39\n",
            "Accuracy (Polynomial Kernel):  78.26\n",
            "F1 (Polynomial Kernel):  78.22\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\outlier.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the target column\n",
        "target_column = \"num\"\n",
        "\n",
        "# Drop the 'age' column if necessary\n",
        "data.drop(columns=['age'], inplace=True)\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Support Vector Classifier (SVM) with the desired kernel and parameters\n",
        "svm_classifier_rbf = SVC(kernel='rbf', gamma=0.5, C=0.1)\n",
        "svm_classifier_poly = SVC(kernel='poly', degree=3, C=1)\n",
        "\n",
        "# Train the classifiers\n",
        "svm_classifier_rbf.fit(X_train, y_train)\n",
        "svm_classifier_poly.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "rbf_pred = svm_classifier_rbf.predict(X_test)\n",
        "poly_pred = svm_classifier_poly.predict(X_test)\n",
        "\n",
        "# Calculate accuracy and F1 score for RBF kernel\n",
        "rbf_accuracy = accuracy_score(y_test, rbf_pred)\n",
        "rbf_f1 = f1_score(y_test, rbf_pred, average='weighted')\n",
        "\n",
        "# Calculate accuracy and F1 score for Polynomial kernel\n",
        "poly_accuracy = accuracy_score(y_test, poly_pred)\n",
        "poly_f1 = f1_score(y_test, poly_pred, average='weighted')\n",
        "\n",
        "# Print results\n",
        "print('Accuracy (RBF Kernel): ', \"%.2f\" % (rbf_accuracy*100))\n",
        "print('F1 (RBF Kernel): ', \"%.2f\" % (rbf_f1*100))\n",
        "print('Accuracy (Polynomial Kernel): ', \"%.2f\" % (poly_accuracy*100))\n",
        "print('F1 (Polynomial Kernel): ', \"%.2f\" % (poly_f1*100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39802932",
      "metadata": {
        "id": "39802932",
        "outputId": "87427a06-0789-42b5-c71c-9409ab8d3949"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected features: ['thalch', 'ca', 'trestbps', 'age', 'slope', 'chol', 'thal', 'oldpeak', 'restecg']\n",
            "0.9258965407807047\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\outlier.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the target column\n",
        "target_column = \"num\"\n",
        "\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "# Initialize an empty set to store selected features\n",
        "selected_features = []\n",
        "\n",
        "# Initialize the classifier\n",
        "classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Define a function to calculate the cross-validated accuracy\n",
        "def evaluate_accuracy(features):\n",
        "    X_subset = X[features]\n",
        "    scores = cross_val_score(classifier, X_subset, y, cv=5)\n",
        "    return scores.mean()\n",
        "\n",
        "# Perform forward feature selection\n",
        "best_accuracy = 0\n",
        "while True:\n",
        "    best_feature = None\n",
        "    for feature in X.columns:\n",
        "        if feature not in selected_features:\n",
        "            selected_features.append(feature)\n",
        "            accuracy = evaluate_accuracy(selected_features)\n",
        "            if accuracy > best_accuracy:\n",
        "                best_accuracy = accuracy\n",
        "                best_feature = feature\n",
        "            selected_features.pop()  # Remove the feature to try the next one\n",
        "    if best_feature is not None:\n",
        "        selected_features.append(best_feature)\n",
        "    else:\n",
        "        break\n",
        "\n",
        "# Print the selected features\n",
        "print(\"Selected features:\", selected_features)\n",
        "print(best_accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ceb810",
      "metadata": {
        "id": "78ceb810",
        "outputId": "944b600d-d3f6-4fd4-dbd0-efadc47268d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          age  sex        cp  trestbps      chol  fbs  restecg    thalch  \\\n",
            "67   0.520833    1  0.666667  0.528302  0.284483    0  0.00000  0.717557   \n",
            "137  0.125000    1  0.000000  0.245283  0.211207    0  0.50000  0.450382   \n",
            "461  0.504182    0  0.000000  0.599141  0.467896    0  0.77509  0.436989   \n",
            "578  0.452121    1  0.000000  0.150943  0.208647    0  0.00000  0.293233   \n",
            "17   0.520833    1  0.000000  0.433962  0.299569    0  0.50000  0.679389   \n",
            "..        ...  ...       ...       ...       ...  ...      ...       ...   \n",
            "760  0.748302    0  0.000000  0.528302  0.285379    0  0.00000  0.340686   \n",
            "765  0.614863    1  0.000000  0.501108  0.252101    0  0.00000  0.475486   \n",
            "751  0.593214    1  0.000000  0.482587  0.318808    0  0.00000  0.515777   \n",
            "626  0.484631    1  0.000000  0.314878  0.373222    0  0.50000  0.402161   \n",
            "160  1.000000    1  0.000000  0.292453  0.439655    0  0.00000  0.694656   \n",
            "\n",
            "     exang   oldpeak  slope        ca      thal  num  \n",
            "67       0  0.258065    1.0  0.000000  1.000000    0  \n",
            "137      1  0.258065    0.5  0.000000  1.000000    1  \n",
            "461      1  0.388773    0.5  0.000000  0.724910    2  \n",
            "578      1  0.075995    0.5  0.254805  0.617792    3  \n",
            "17       0  0.193548    1.0  0.000000  0.500000    0  \n",
            "..     ...       ...    ...       ...       ...  ...  \n",
            "760      0  0.181007    0.5  1.000000  1.000000    4  \n",
            "765      1  0.393483    0.5  0.333333  0.707215    4  \n",
            "751      1  0.256738    0.5  0.498286  1.000000    4  \n",
            "626      1  0.582408    0.5  0.719490  1.000000    3  \n",
            "160      1  0.000000    1.0  1.000000  0.500000    4  \n",
            "\n",
            "[800 rows x 14 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\heart_disease_uci.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "columns_with_missing_values = ['trestbps', 'chol', 'thalch', 'oldpeak','fbs', 'restecg', 'ca', 'slope', 'thal']\n",
        "# Drop missing values from specified columns\n",
        "df.dropna(subset=columns_with_missing_values, inplace=True)\n",
        "# Encoding string to numerical\n",
        "label_encoder = LabelEncoder()\n",
        "columns=['sex','cp','fbs','restecg','exang','slope','thal']\n",
        "for column in columns:\n",
        "    df[column] = label_encoder.fit_transform(df[column])\n",
        "# Normalization\n",
        "columns_tonormalize = ['age','cp','trestbps', 'chol', 'restecg', 'thalch', 'oldpeak', 'slope', 'ca', 'thal']\n",
        "scaler = MinMaxScaler()\n",
        "df[columns_tonormalize] = scaler.fit_transform(df[columns_tonormalize])\n",
        "# Outlier removal\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "outliers = isolation_forest.fit_predict(df)\n",
        "df_filtered = df[outliers != -1]\n",
        "# SMOTE - Oversampling\n",
        "X = df.drop('num', axis=1)\n",
        "y = df['num']\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "resampled_df['num'] = y_resampled\n",
        "resampled_df = resampled_df.sample(frac=1)\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\resampled_dataset.csv'\n",
        "resampled_df.to_csv(new_file_path, index=False)\n",
        "print(resampled_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b49b0d7",
      "metadata": {
        "id": "3b49b0d7",
        "outputId": "6d34fc9c-43a0-45de-ba62-e93dc910bfd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.24.3)\n",
            "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.11.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.1.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5739c9b5",
      "metadata": {
        "id": "5739c9b5",
        "outputId": "f67c37e3-1d96-4aea-d680-67a68771f340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Columns selected by SU feature selection method:\n",
            "['age', 'sex', 'cp', 'trestbps', 'chol', 'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from math import log2\n",
        "\n",
        "def calculate_entropy(data):\n",
        "    value_counts = data.value_counts(normalize=True)\n",
        "    entropy = -sum(p * log2(p) for p in value_counts)\n",
        "    return entropy\n",
        "\n",
        "def calculate_joint_entropy(x, y):\n",
        "    joint_data = pd.concat([x, y], axis=1)\n",
        "    joint_entropy = calculate_entropy(joint_data)\n",
        "    return joint_entropy\n",
        "\n",
        "def calculate_su(x, y):\n",
        "    entropy_x = calculate_entropy(x)\n",
        "    entropy_y = calculate_entropy(y)\n",
        "    joint_entropy_xy = calculate_joint_entropy(x, y)\n",
        "    su = 2 * (entropy_x - joint_entropy_xy) / (entropy_x + entropy_y)\n",
        "    return su\n",
        "\n",
        "def feature_selection_by_su(data, target_column):\n",
        "\n",
        "    su_values = {} #  su between feature and target\n",
        "    for column in data.columns:\n",
        "        if column != target_column:\n",
        "            su_values[column] = calculate_su(data[column], data[target_column])\n",
        "\n",
        "    selected_columns = []\n",
        "\n",
        "#    to remove less correlated features\n",
        "    sorted_su_values = sorted(su_values.items(), key=lambda x: x[1], reverse=True)\n",
        "    last_item_su_value = sorted_su_values[-1][1]\n",
        "    if -0.02 <= last_item_su_value <= 0:\n",
        "        column_to_drop = sorted_su_values[-1][0]\n",
        "        data.drop(columns=[column_to_drop], inplace=True)\n",
        "\n",
        "#    su between feature to feature\n",
        "    su_values = {}\n",
        "    for column in data.columns:\n",
        "        if column != target_column:\n",
        "            su_values[column] = []\n",
        "            for other_column in data.columns:\n",
        "                if other_column != column and other_column != target_column:\n",
        "                    su_value = calculate_su(data[column], data[other_column])\n",
        "                    su_values[column].append((other_column, su_value))\n",
        "\n",
        "#    for redundant features removal\n",
        "    tosort = []\n",
        "    highly_correlated_features = []\n",
        "    for feature, correlations in su_values.items():\n",
        "        for correlation in correlations:\n",
        "            other_feature, su_value = correlation\n",
        "            tosort.append((feature, other_feature, su_value))\n",
        "    sorted_unique_tosort = sorted(tosort, key=lambda x: abs(x[2]))\n",
        "    index_map = {feature: i for i, (feature, _) in enumerate(sorted_su_values)}\n",
        "    features_to_remove = []\n",
        "    for i in range(-3, 0):\n",
        "        f1, f2, _ = sorted_unique_tosort[i]\n",
        "        if f1 in [item[0] for item in sorted_su_values] and f2 in [item[0] for item in sorted_su_values]:\n",
        "            f1_index = index_map[f1]\n",
        "            f2_index = index_map[f2]\n",
        "            feature_to_remove = f1 if f1_index > f2_index else f2\n",
        "            features_to_remove.append(feature_to_remove)\n",
        "    if features_to_remove:\n",
        "        max_index = max(index_map[feature] for feature in features_to_remove)\n",
        "        feature_to_remove = next(feature for feature, index in index_map.items() if index == max_index)\n",
        "        data.drop(columns=[feature_to_remove], inplace=True)\n",
        "    selected_columns.extend(data.columns)\n",
        "    return data, selected_columns\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smoteagain.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "target_column = 'num'\n",
        "updated_data, selected_columns = feature_selection_by_su(data, target_column)\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv'\n",
        "updated_data.to_csv(new_file_path, index=False)\n",
        "\n",
        "print(\"Columns selected by SU feature selection method:\")\n",
        "print(selected_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e89cdd76",
      "metadata": {
        "id": "e89cdd76"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}