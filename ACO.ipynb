{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12b3953e",
      "metadata": {
        "id": "12b3953e",
        "outputId": "fd87186b-a3d1-40af-8cdf-4ba4b82d4802"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9347826086956522\n",
            "Recall: 0.9347826086956522\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# # Load the dataset\n",
        "# file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\outlier.csv'\n",
        "# data = pd.read_csv(file_path)\n",
        "\n",
        "# # Define the target column\n",
        "# target_column = \"num\"\n",
        "# # data.drop(columns=['age'], inplace=True)\n",
        "# # Split the dataset into features (X) and target variable (y)\n",
        "# X = data.drop(columns=[target_column])\n",
        "# y = data[target_column]\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Initialize the Extra Trees Classifier\n",
        "# et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# # Train the classifier\n",
        "# et_classifier.fit(X_train, y_train)\n",
        "\n",
        "# # Predict on the test set\n",
        "# y_pred = et_classifier.predict(X_test)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# # Calculate recall\n",
        "# # Choose one of the average options: 'micro', 'macro', or 'weighted'\n",
        "# recall = recall_score(y_test, y_pred, average='micro')\n",
        "# print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cf6dcf9",
      "metadata": {
        "id": "2cf6dcf9",
        "outputId": "56167375-f103-48e2-cef3-f8b1e137d377"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.927536231884058\n",
            "Recall: 0.927536231884058\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# # Load the dataset\n",
        "# file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv'\n",
        "# data = pd.read_csv(file_path)\n",
        "\n",
        "# # Define the target column\n",
        "# target_column = \"num\"\n",
        "# # data.drop(columns=['age'], inplace=True)\n",
        "# # Split the dataset into features (X) and target variable (y)\n",
        "# X = data.drop(columns=[target_column])\n",
        "# y = data[target_column]\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Initialize the Extra Trees Classifier\n",
        "# et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# # Train the classifier\n",
        "# et_classifier.fit(X_train, y_train)\n",
        "\n",
        "# # Predict on the test set\n",
        "# y_pred = et_classifier.predict(X_test)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# # Calculate recall\n",
        "# # Choose one of the average options: 'micro', 'macro', or 'weighted'\n",
        "# recall = recall_score(y_test, y_pred, average='micro')\n",
        "# print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24df7a03",
      "metadata": {
        "id": "24df7a03",
        "outputId": "ab933373-eda6-45ff-81f5-ddd997c381e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9710144927536232\n",
            "Recall: 0.9710144927536232\n"
          ]
        }
      ],
      "source": [
        "#2\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smoteagain.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Define the target column\n",
        "target_column = \"num\"\n",
        "# data.drop(columns=['age'], inplace=True)\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Extra Trees Classifier\n",
        "et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "# print(et_classifier.get_params())\n",
        "# Train the classifier\n",
        "et_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = et_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate recall\n",
        "# Choose one of the average options: 'micro', 'macro', or 'weighted'\n",
        "recall = recall_score(y_test, y_pred, average='micro')\n",
        "print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60f0efc7",
      "metadata": {
        "id": "60f0efc7"
      },
      "outputs": [],
      "source": [
        "#1\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from imblearn.over_sampling import SMOTE\n",
        "# file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\outlier.csv'\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smoteagain.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "X = df.drop('num', axis=1)\n",
        "y = df['num']\n",
        "\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "resampled_df['num'] = y_resampled\n",
        "resampled_df = resampled_df.sample(frac=1)\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "outliers = isolation_forest.fit_predict(resampled_df)\n",
        "df_filtered = resampled_df[outliers != -1]\n",
        "\n",
        "\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smoteagain.csv'\n",
        "df_filtered.to_csv(new_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb28475",
      "metadata": {
        "id": "beb28475",
        "outputId": "b89b2502-5c46-41bf-d4e9-691d64d21645"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.24.3)\n",
            "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.11.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.1.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efbed139",
      "metadata": {
        "id": "efbed139",
        "outputId": "f74cd1f0-687f-47c8-8b47-2f8793e9f002"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9637681159420289\n",
            "Recall: 0.9637681159420289\n"
          ]
        }
      ],
      "source": [
        "#3\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "# data.drop(columns=['fbs'], inplace=True)\n",
        "# Define the target column\n",
        "target_column = \"num\"\n",
        "# data.drop(columns=['age'], inplace=True)\n",
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Extra Trees Classifier\n",
        "et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "et_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = et_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Calculate recall\n",
        "# Choose one of the average options: 'micro', 'macro', or 'weighted'\n",
        "recall = recall_score(y_test, y_pred, average='micro')\n",
        "print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc4291a",
      "metadata": {
        "id": "2fc4291a",
        "outputId": "8fd1add5-7f30-4f86-cc85-adb9b9184de7"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m rfe \u001b[38;5;241m=\u001b[39m RFECV(estimator\u001b[38;5;241m=\u001b[39met_classifier, step\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Fit RFECV\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m rfe\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Get selected features\u001b[39;00m\n\u001b[0;32m     31\u001b[0m selected_features \u001b[38;5;241m=\u001b[39m X_train\u001b[38;5;241m.\u001b[39mcolumns[rfe\u001b[38;5;241m.\u001b[39msupport_]\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:752\u001b[0m, in \u001b[0;36mRFECV.fit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    749\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    750\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[1;32m--> 752\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[0;32m    753\u001b[0m     func(rfe, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, X, y, train, test, scorer)\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    755\u001b[0m )\n\u001b[0;32m    757\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    758\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:753\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    749\u001b[0m     parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs)\n\u001b[0;32m    750\u001b[0m     func \u001b[38;5;241m=\u001b[39m delayed(_rfe_single_fit)\n\u001b[0;32m    752\u001b[0m scores \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m--> 753\u001b[0m     func(rfe, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator, X, y, train, test, scorer)\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m train, test \u001b[38;5;129;01min\u001b[39;00m cv\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n\u001b[0;32m    755\u001b[0m )\n\u001b[0;32m    757\u001b[0m scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(scores)\n\u001b[0;32m    758\u001b[0m scores_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(scores, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:35\u001b[0m, in \u001b[0;36m_rfe_single_fit\u001b[1;34m(rfe, estimator, X, y, train, test, scorer)\u001b[0m\n\u001b[0;32m     33\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, train)\n\u001b[0;32m     34\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m _safe_split(estimator, X, y, test, train)\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rfe\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[0;32m     36\u001b[0m     X_train,\n\u001b[0;32m     37\u001b[0m     y_train,\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m estimator, features: _score(\n\u001b[0;32m     39\u001b[0m         \u001b[38;5;66;03m# TODO(SLEP6): pass score_params here\u001b[39;00m\n\u001b[0;32m     40\u001b[0m         estimator,\n\u001b[0;32m     41\u001b[0m         X_test[:, features],\n\u001b[0;32m     42\u001b[0m         y_test,\n\u001b[0;32m     43\u001b[0m         scorer,\n\u001b[0;32m     44\u001b[0m         score_params\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     45\u001b[0m     ),\n\u001b[0;32m     46\u001b[0m )\u001b[38;5;241m.\u001b[39mscores_\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    315\u001b[0m     estimator,\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    318\u001b[0m )\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:478\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[1;32m--> 478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:479\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarm_start \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;66;03m# We draw from the random state to get the random state we\u001b[39;00m\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# would have got if we hadn't used a warm_start.\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_))\n\u001b[0;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    481\u001b[0m ]\n\u001b[0;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m    489\u001b[0m trees \u001b[38;5;241m=\u001b[39m Parallel(\n\u001b[0;32m    490\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs,\n\u001b[0;32m    491\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    507\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trees)\n\u001b[0;32m    508\u001b[0m )\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_base.py:141\u001b[0m, in \u001b[0;36mBaseEnsemble._make_estimator\u001b[1;34m(self, append, random_state)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_estimator\u001b[39m(\u001b[38;5;28mself\u001b[39m, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    136\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make and configure a copy of the `estimator_` attribute.\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \n\u001b[0;32m    138\u001b[0m \u001b[38;5;124;03m    Warning: This method should be used to properly instantiate new\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    sub-estimators.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 141\u001b[0m     estimator \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_)\n\u001b[0;32m    142\u001b[0m     estimator\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{p: \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_params})\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m random_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:90\u001b[0m, in \u001b[0;36mclone\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Construct a new unfitted estimator with the same parameters.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03mClone does a deep copy of the model in an estimator\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03mFalse\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sklearn_clone__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39misclass(estimator):\n\u001b[1;32m---> 90\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator\u001b[38;5;241m.\u001b[39m__sklearn_clone__()\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(estimator, safe\u001b[38;5;241m=\u001b[39msafe)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:296\u001b[0m, in \u001b[0;36mBaseEstimator.__sklearn_clone__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sklearn_clone__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _clone_parametrized(\u001b[38;5;28mself\u001b[39m)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:131\u001b[0m, in \u001b[0;36m_clone_parametrized\u001b[1;34m(estimator, safe)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m params_set \u001b[38;5;241m=\u001b[39m new_object\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# quick sanity check of the parameters of the clone\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m new_object_params:\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:243\u001b[0m, in \u001b[0;36mBaseEstimator.get_params\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;124;03mGet parameters for this estimator.\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    Parameter names mapped to their values.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    242\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_param_names():\n\u001b[0;32m    244\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mtype\u001b[39m):\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:208\u001b[0m, in \u001b[0;36mBaseEstimator._get_param_names\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m    206\u001b[0m \u001b[38;5;66;03m# introspect the constructor arguments to find the model parameters\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;66;03m# to represent\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m init_signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(init)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Consider the constructor parameters excluding 'self'\u001b[39;00m\n\u001b[0;32m    210\u001b[0m parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    211\u001b[0m     p\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m init_signature\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;241m!=\u001b[39m p\u001b[38;5;241m.\u001b[39mVAR_KEYWORD\n\u001b[0;32m    214\u001b[0m ]\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\inspect.py:3280\u001b[0m, in \u001b[0;36msignature\u001b[1;34m(obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[0;32m   3278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msignature\u001b[39m(obj, \u001b[38;5;241m*\u001b[39m, follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   3279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Signature\u001b[38;5;241m.\u001b[39mfrom_callable(obj, follow_wrapped\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[0;32m   3281\u001b[0m                                    \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\inspect.py:3028\u001b[0m, in \u001b[0;36mSignature.from_callable\u001b[1;34m(cls, obj, follow_wrapped, globals, locals, eval_str)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_callable\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj, \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m   3026\u001b[0m                   follow_wrapped\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, eval_str\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 3028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_callable(obj, sigcls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   3029\u001b[0m                                     follow_wrapper_chains\u001b[38;5;241m=\u001b[39mfollow_wrapped,\n\u001b[0;32m   3030\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\inspect.py:2516\u001b[0m, in \u001b[0;36m_signature_from_callable\u001b[1;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[0;32m   2511\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mreplace(parameters\u001b[38;5;241m=\u001b[39mnew_params)\n\u001b[0;32m   2513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m isfunction(obj) \u001b[38;5;129;01mor\u001b[39;00m _signature_is_functionlike(obj):\n\u001b[0;32m   2514\u001b[0m     \u001b[38;5;66;03m# If it's a pure Python function, or an object that is duck type\u001b[39;00m\n\u001b[0;32m   2515\u001b[0m     \u001b[38;5;66;03m# of a Python function (Cython functions, for instance), then:\u001b[39;00m\n\u001b[1;32m-> 2516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_function(sigcls, obj,\n\u001b[0;32m   2517\u001b[0m                                     skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg,\n\u001b[0;32m   2518\u001b[0m                                     \u001b[38;5;28mglobals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mglobals\u001b[39m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlocals\u001b[39m, eval_str\u001b[38;5;241m=\u001b[39meval_str)\n\u001b[0;32m   2520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _signature_is_builtin(obj):\n\u001b[0;32m   2521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _signature_from_builtin(sigcls, obj,\n\u001b[0;32m   2522\u001b[0m                                    skip_bound_arg\u001b[38;5;241m=\u001b[39mskip_bound_arg)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\inspect.py:2423\u001b[0m, in \u001b[0;36m_signature_from_function\u001b[1;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     parameters\u001b[38;5;241m.\u001b[39mappend(Parameter(name, annotation\u001b[38;5;241m=\u001b[39mannotation,\n\u001b[0;32m   2419\u001b[0m                                 kind\u001b[38;5;241m=\u001b[39m_VAR_KEYWORD))\n\u001b[0;32m   2421\u001b[0m \u001b[38;5;66;03m# Is 'func' is a pure Python function - don't validate the\u001b[39;00m\n\u001b[0;32m   2422\u001b[0m \u001b[38;5;66;03m# parameters list (for correct order and defaults), it should be OK.\u001b[39;00m\n\u001b[1;32m-> 2423\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(parameters,\n\u001b[0;32m   2424\u001b[0m            return_annotation\u001b[38;5;241m=\u001b[39mannotations\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn\u001b[39m\u001b[38;5;124m'\u001b[39m, _empty),\n\u001b[0;32m   2425\u001b[0m            __validate_parameters__\u001b[38;5;241m=\u001b[39mis_duck_function)\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\inspect.py:3019\u001b[0m, in \u001b[0;36mSignature.__init__\u001b[1;34m(self, parameters, return_annotation, __validate_parameters__)\u001b[0m\n\u001b[0;32m   3017\u001b[0m             params[name] \u001b[38;5;241m=\u001b[39m param\n\u001b[0;32m   3018\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3019\u001b[0m         params \u001b[38;5;241m=\u001b[39m OrderedDict((param\u001b[38;5;241m.\u001b[39mname, param) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m parameters)\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mMappingProxyType(params)\n\u001b[0;32m   3022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_annotation \u001b[38;5;241m=\u001b[39m return_annotation\n",
            "File \u001b[1;32mC:\\Users\\Thasneem.DESKTOP-MET98BF\\anaconda3\\Lib\\inspect.py:3019\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   3017\u001b[0m             params[name] \u001b[38;5;241m=\u001b[39m param\n\u001b[0;32m   3018\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3019\u001b[0m         params \u001b[38;5;241m=\u001b[39m OrderedDict((param\u001b[38;5;241m.\u001b[39mname, param) \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m parameters)\n\u001b[0;32m   3021\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameters \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mMappingProxyType(params)\n\u001b[0;32m   3022\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_annotation \u001b[38;5;241m=\u001b[39m return_annotation\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.feature_selection import RFECV\n",
        "# from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# # Load the dataset\n",
        "# file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smoteagain.csv'\n",
        "# data = pd.read_csv(file_path)\n",
        "\n",
        "# # Define the target column\n",
        "# target_column = \"num\"\n",
        "\n",
        "# # Split the dataset into features (X) and target variable (y)\n",
        "# X = data.drop(columns=[target_column])\n",
        "# y = data[target_column]\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Initialize the Extra Trees Classifier\n",
        "# et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# # Initialize RFECV with min_features_to_select parameter\n",
        "# rfe = RFECV(estimator=et_classifier, step=1, cv=5, scoring='accuracy')\n",
        "\n",
        "# # Fit RFECV\n",
        "# rfe.fit(X_train, y_train)\n",
        "\n",
        "# # Get selected features\n",
        "# selected_features = X_train.columns[rfe.support_]\n",
        "# print(selected_features)\n",
        "# # Train the classifier with selected features\n",
        "# et_classifier.fit(X_train[selected_features], y_train)\n",
        "\n",
        "# # Predict on the test set with selected features\n",
        "# y_pred = et_classifier.predict(X_test[selected_features])\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# # Calculate recall\n",
        "# # Choose one of the average options: 'micro', 'macro', or 'weighted'\n",
        "# recall = recall_score(y_test, y_pred, average='micro')\n",
        "# print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4b50d2",
      "metadata": {
        "id": "0e4b50d2",
        "outputId": "8c430f78-d864-48a4-f93d-9b5be69efaf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          age  sex        cp  trestbps      chol  restecg    thalch  exang  \\\n",
            "0    0.489408    1  0.299439  0.316918  0.326599      0.0  0.749192      0   \n",
            "1    0.545558    1  0.000000  0.380050  0.366284      0.0  0.299136      1   \n",
            "2    0.615378    1  0.606874  0.197733  0.276693      0.0  0.694964      0   \n",
            "3    0.135869    1  0.000000  0.252442  0.215091      0.5  0.442779      1   \n",
            "4    0.505407    1  0.000000  0.589802  0.372818      0.0  0.519713      0   \n",
            "..        ...  ...       ...       ...       ...      ...       ...    ...   \n",
            "683  0.644220    1  0.000000  0.346804  0.245977      0.5  0.673315      0   \n",
            "684  0.297821    1  0.016412  0.519012  0.318826      0.5  0.763359      0   \n",
            "685  0.562500    1  0.000000  0.245283  0.000000      0.5  0.374046      1   \n",
            "686  0.637624    1  0.309789  0.301115  0.206336      0.0  0.554102      0   \n",
            "687  0.676074    1  0.763435  0.428485  0.243879      0.0  0.683327      0   \n",
            "\n",
            "      oldpeak     slope        ca      thal  \n",
            "0    0.276246  0.724579  0.299439  1.000000  \n",
            "1    0.252751  0.500000  0.333333  0.604411  \n",
            "2    0.395993  0.500000  0.363230  0.910311  \n",
            "3    0.256535  0.500000  0.007905  1.000000  \n",
            "4    0.230092  0.500000  0.217301  1.000000  \n",
            "..        ...       ...       ...       ...  \n",
            "683  0.005793  1.000000  0.333333  1.000000  \n",
            "684  0.234788  1.000000  0.000000  0.500000  \n",
            "685  0.241935  0.500000  0.000000  1.000000  \n",
            "686  0.498201  0.500000  0.333333  0.732342  \n",
            "687  0.408953  0.500000  0.096769  0.500000  \n",
            "\n",
            "[688 rows x 12 columns]\n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9710144927536232  \n",
            "0.9710144927536232  \n",
            "0.9710144927536232  \n",
            "0.9565217391304348  \n",
            "0.9710144927536232  \n",
            "FINAL:\n",
            "[99.15617651668076, 11.052068050698782, 2.116238347387579, 1.5726935839847247, 0.2783262722714165]\n",
            "0.9710144927536232\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.PSO at 0x2515fff1cd0>"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#4\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv')\n",
        "\n",
        "# Separate features and target variable\n",
        "X = data.drop('num', axis=1)\n",
        "y = data['num']\n",
        "print(X)\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "class Particle:\n",
        "    def __init__(self, params):\n",
        "        self.params_i = []           # particle parameters\n",
        "        self.velocity_i = []         # particle velocity\n",
        "        self.pos_best_i = []         # best position individual\n",
        "        self.err_best_i = -1         # best error individual\n",
        "        self.err_i = -1              # error individual\n",
        "\n",
        "        for param in params:\n",
        "            self.velocity_i.append(random.uniform(-1, 1))\n",
        "            self.params_i.append(param)\n",
        "\n",
        "    # evaluate current fitness\n",
        "    def evaluate(self, costFunc):\n",
        "        self.err_i = costFunc(self.params_i)\n",
        "\n",
        "        # check to see if the current position is an individual best\n",
        "        if self.err_i >= self.err_best_i:\n",
        "            print(self.err_i,\" \")\n",
        "            self.pos_best_i = self.params_i\n",
        "            self.err_best_i = self.err_i\n",
        "\n",
        "    # update new particle velocity\n",
        "    def update_velocity(self, pos_best_g):\n",
        "        w = 0.5       # constant inertia weight (how much to weigh the previous velocity)\n",
        "        c1 = 1        # cognitive constant\n",
        "        c2 = 2        # social constant\n",
        "\n",
        "        for i in range(len(self.params_i)):\n",
        "            r1 = random.random()\n",
        "            r2 = random.random()\n",
        "\n",
        "            vel_cognitive = c1 * r1 * (self.pos_best_i[i] - self.params_i[i])\n",
        "            vel_social = c2 * r2 * (pos_best_g[i] - self.params_i[i])\n",
        "            self.velocity_i[i] = w * self.velocity_i[i] + vel_cognitive + vel_social\n",
        "\n",
        "    # update the particle position based off new velocity updates\n",
        "    def update_position(self, bounds):\n",
        "        for i in range(len(self.params_i)):\n",
        "            self.params_i[i] = self.params_i[i] + self.velocity_i[i]\n",
        "\n",
        "            # adjust maximum position if necessary\n",
        "            if self.params_i[i] > bounds[i][1]:\n",
        "                self.params_i[i] = bounds[i][1]\n",
        "\n",
        "            # adjust minimum position if necessary\n",
        "            if self.params_i[i] < bounds[i][0]:\n",
        "                self.params_i[i] = bounds[i][0]\n",
        "\n",
        "class PSO:\n",
        "    def __init__(self, costFunc, params, bounds, num_particles, maxiter):\n",
        "        global num_dimensions\n",
        "\n",
        "        num_dimensions = len(params)\n",
        "        err_best_g = -1                   # best error for group\n",
        "        pos_best_g = []                   # best position for group\n",
        "\n",
        "        # establish the swarm\n",
        "        swarm = []\n",
        "        for i in range(num_particles):\n",
        "            swarm.append(Particle(params))\n",
        "\n",
        "        # begin optimization loop\n",
        "        i = 0\n",
        "        while i < maxiter:\n",
        "            # cycle through particles in swarm and evaluate fitness\n",
        "            for j in range(num_particles):\n",
        "                swarm[j].evaluate(costFunc)\n",
        "\n",
        "                # determine if current particle is the best (globally)\n",
        "                if swarm[j].err_i >= err_best_g:\n",
        "                    pos_best_g = list(swarm[j].pos_best_i)\n",
        "                    err_best_g = float(swarm[j].err_i)\n",
        "\n",
        "            # cycle through swarm and update velocities and position\n",
        "            for j in range(num_particles):\n",
        "                swarm[j].update_velocity(pos_best_g)\n",
        "                swarm[j].update_position(bounds)\n",
        "            i += 1\n",
        "\n",
        "        # print final results\n",
        "        print('FINAL:')\n",
        "        print(pos_best_g)\n",
        "        print(err_best_g)\n",
        "\n",
        "def evaluate_params(params):\n",
        "    # Evaluate Extra Trees Classifier with given hyperparameters\n",
        "#     print(params)\n",
        "    extra_trees_clf = ExtraTreesClassifier(n_estimators=int(params[0]), max_depth=int(params[1]),\n",
        "                                           min_samples_split=int(params[2]), min_samples_leaf=int(params[3]),\n",
        "                                           max_features=params[4], random_state=42)\n",
        "    extra_trees_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the testing data\n",
        "    y_pred = extra_trees_clf.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "initial = [100, 10, 2, 1, 0.5]  # initial starting location [n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features]\n",
        "bounds = [(10, 500), (1, 50), (2, 20), (1, 10), (0.1, 0.9)]  # input bounds [(param_min, param_max), ...]\n",
        "PSO(evaluate_params, initial, bounds, num_particles=15, maxiter=10)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777daeea",
      "metadata": {
        "id": "777daeea",
        "outputId": "6350fc9b-83c8-4e5e-b1d2-c66a0f4353af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Information(Samples, Attributes): (688, 13)\n",
            "\n",
            "Attribute Selection based on Ant Colony Optimization:\n",
            "Designed to select attributes from a given dataset through ACO adopting the cosine similarity between attribute pairs as weight. The performance of the subsets (accuracy) through a modeling will be evaluated and at the end the set with the highest value will be presented. The pheromone update and the probability rule were developed according to the Ant-System algorithm\n",
            "--------------------\n",
            "Parameters of ACO:\n",
            "Number of Ants:\t\t\t\t\t13\n",
            "Rate of evaporation:\t\t\t\t\t0.2\n",
            "Alpha Heuristic(importance of pheromone):\t\t1\n",
            "Beta Heuristic(importance of heuristic information):\t1\n",
            "Number of iteration:\t\t\t\t\t20\n",
            "Number of Attributes to be selected:\t\t\t10\n",
            "--------------------\n",
            "1.0\n",
            "Solution(sub-set) of attributes that presented the highest accuracy over 20 iterations:\n",
            " \n",
            "Accuracy for Extra Trees:  100.0\n"
          ]
        }
      ],
      "source": [
        "# import time\n",
        "# from scipy import spatial\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "# import math\n",
        "# import random\n",
        "# class Accuracy:\n",
        "\n",
        "#     def __init__(self, iteration):\n",
        "#         self.solution = []\n",
        "#         self.accuracy = None\n",
        "\n",
        "#     def setSolution(self, solution, accuracy):\n",
        "#         self.solution = solution\n",
        "#         self.accuracy = accuracy\n",
        "\n",
        "#     def obtainAccuracy_final(self):\n",
        "#         return self.accuracy\n",
        "\n",
        "#     def obtainSolution_final(self):\n",
        "#         return self.solution\n",
        "\n",
        "# class Edge:\n",
        "\n",
        "#     def __init__(self, origin, destination, cost):\n",
        "#         self.origin = origin\n",
        "#         self.destination = destination\n",
        "#         self.cost = cost\n",
        "#         self.pheromone = None\n",
        "\n",
        "#     def obtainOrigin(self):\n",
        "#         return self.origin\n",
        "\n",
        "#     def obtainDestination(self):\n",
        "#         return self.destination\n",
        "\n",
        "#     def obtainCost(self):\n",
        "#         return self.cost\n",
        "\n",
        "#     def obtainPheromone(self):\n",
        "#         return self.pheromone\n",
        "\n",
        "#     def setPheromone(self, pheromone):\n",
        "#         self.pheromone = pheromone\n",
        "\n",
        "# class Graph:\n",
        "\n",
        "#     def __init__(self, num_vertices):\n",
        "#         self.num_vertices = num_vertices\n",
        "#         self.edges = {}\n",
        "#         self.neighbors = {}\n",
        "#         self.vertices = {}\n",
        "\n",
        "#     def addEdge(self, origin, destination, cost):\n",
        "#         edge = Edge(origin=origin, destination=destination, cost=cost)\n",
        "#         self.edges[(origin, destination)] = edge\n",
        "#         if origin not in self.neighbors:\n",
        "#             self.neighbors[origin] = [destination]\n",
        "#         else:\n",
        "#             self.neighbors[origin].append(destination)\n",
        "\n",
        "#     def obtainCostEdge(self, origin, destination):\n",
        "#         return self.edges[(origin, destination)].obtainCost()\n",
        "\n",
        "#     def obtainPheromoneEdge(self, origin, destination):\n",
        "#         return self.edges[(origin, destination)].obtainPheromone()\n",
        "\n",
        "#     def setPheromoneEdge(self, origin, destination, pheromone):\n",
        "#         self.edges[(origin, destination)].setPheromone(pheromone)\n",
        "\n",
        "#     def obtainCostPath(self, path):\n",
        "#         cost = 0\n",
        "#         for i in range(self.num_vertices - 1):\n",
        "#             cost += self.obtainCostEdge(path[i], path[i + 1])\n",
        "\n",
        "#         cost += self.obtainCostEdge(path[-1], path[0])\n",
        "#         return cost\n",
        "\n",
        "\n",
        "# class GraphComplete(Graph):\n",
        "\n",
        "#     def generate(self, matrix):\n",
        "#         for i in range(self.num_vertices):\n",
        "#             for j in range(self.num_vertices):\n",
        "#                 if i != j:\n",
        "#                     weight = matrix[i][j]\n",
        "#                     self.addEdge(i, j, weight)\n",
        "\n",
        "\n",
        "# class Ant:\n",
        "\n",
        "#     def __init__(self, city):\n",
        "#         self.city = city\n",
        "#         self.solution = []\n",
        "#         self.cost = None\n",
        "#         self.accuracy = None\n",
        "\n",
        "#     def obtainCity(self):\n",
        "#         return self.city\n",
        "\n",
        "#     def setCity(self, city):\n",
        "#         self.city = city\n",
        "\n",
        "#     def obtainSolution(self):\n",
        "#         return self.solution\n",
        "\n",
        "#     def setSolution(self, solution, accuracy):\n",
        "\n",
        "#         if not self.accuracy:\n",
        "#             self.solution = solution[:]\n",
        "#             self.accuracy = accuracy\n",
        "#         else:\n",
        "#             if accuracy > self.accuracy:\n",
        "#                 self.solution = solution[:]\n",
        "#                 self.accuracy = accuracy\n",
        "\n",
        "#     def obtainCostSolution(self):\n",
        "#         return self.cost\n",
        "\n",
        "#     def obtainAccuracy(self):\n",
        "#         return self.accuracy\n",
        "\n",
        "# class ACO:\n",
        "\n",
        "#     def __init__(self, graph, num_ants, alpha=1.0, beta=5.0, iterations=10,\n",
        "#                  evaporation=0.2, num_FS=8):\n",
        "#         self.graph = graph\n",
        "#         self.num_ants = num_ants\n",
        "#         self.alpha = alpha\n",
        "#         self.beta = beta\n",
        "#         self.iterations = iterations\n",
        "#         self.evaporation = evaporation\n",
        "#         self.num_FS = num_FS\n",
        "#         self.ants = []\n",
        "#         self.accuracies = []\n",
        "\n",
        "#         list_cities = [city for city in range(self.graph.num_vertices)]\n",
        "#         for k in range(self.num_ants):\n",
        "#             city_ant = random.choice(list_cities)\n",
        "#             list_cities.remove(city_ant)\n",
        "#             self.ants.append(Ant(city=city_ant))\n",
        "#             if not list_cities:\n",
        "#                 list_cities = [city for city in range(self.graph.num_vertices)]\n",
        "\n",
        "#         cost_greedy = 0.0\n",
        "#         vertex_initial = random.randint(1, graph.num_vertices)\n",
        "#         vertex_current = vertex_initial\n",
        "#         visited = [vertex_current]\n",
        "#         while True:\n",
        "#             neighbors = (self.graph.neighbors[vertex_current])[:]\n",
        "#             (costs, selected) = ([], {})\n",
        "#             for neighbor in neighbors:\n",
        "#                 if neighbor not in visited:\n",
        "#                     cost = self.graph.obtainCostEdge(vertex_current, neighbor)\n",
        "#                     selected[cost] = neighbor\n",
        "#                     costs.append(cost)\n",
        "#             if len(visited) == self.graph.num_vertices:\n",
        "#                 break\n",
        "#             min_cost = min(costs)\n",
        "\n",
        "\n",
        "#             cost_greedy += min_cost\n",
        "#             vertex_current = selected[min_cost]\n",
        "#             visited.append(vertex_current)\n",
        "#         cost_greedy += self.graph.obtainCostEdge(visited[-1], vertex_initial)\n",
        "\n",
        "#         for key_edge in self.graph.edges:\n",
        "#             pheromone = 1.0 / (self.graph.num_vertices * cost_greedy)\n",
        "#             self.graph.setPheromoneEdge(key_edge[0], key_edge[1], pheromone)\n",
        "\n",
        "#     def print(self):\n",
        "\n",
        "#         string = \"\\nAttribute Selection based on Ant Colony Optimization:\"\n",
        "#         string += \"\\nDesigned to select attributes from a given dataset through ACO adopting the cosine similarity between attribute pairs as weight. The performance of the subsets (accuracy) through a modeling will be evaluated and at the end the set with the highest value will be presented. The pheromone update and the probability rule were developed according to the Ant-System algorithm\"\n",
        "#         string += \"\\n--------------------\"\n",
        "#         string += \"\\nParameters of ACO:\"\n",
        "#         string += \"\\nNumber of Ants:\\t\\t\\t\\t\\t{}\".format(self.num_ants)\n",
        "#         string += \"\\nRate of evaporation:\\t\\t\\t\\t\\t{}\".format(self.evaporation)\n",
        "#         string += \"\\nAlpha Heuristic(importance of pheromone):\\t\\t{}\".format(self.alpha)\n",
        "#         string += \"\\nBeta Heuristic(importance of heuristic information):\\t{}\".format(self.beta)\n",
        "#         string += \"\\nNumber of iteration:\\t\\t\\t\\t\\t{}\".format(self.iterations)\n",
        "#         string += \"\\nNumber of Attributes to be selected:\\t\\t\\t{}\".format(self.num_FS)\n",
        "#         string += \"\\n--------------------\"\n",
        "\n",
        "#         print(string)\n",
        "\n",
        "\n",
        "#     def run(self, data_bank, target):\n",
        "\n",
        "#         for it in range(self.iterations):\n",
        "\n",
        "#             cities_visited = []\n",
        "\n",
        "#             for k in range(self.num_ants):\n",
        "#                 cities = [self.ants[k].obtainCity()]\n",
        "#                 cities_visited.append(cities)\n",
        "\n",
        "#             for k in range(self.num_ants):\n",
        "\n",
        "#                 for i in range(1, self.graph.num_vertices):\n",
        "\n",
        "#                     cities_not_visited = list(set(self.graph.neighbors[self.ants[k].obtainCity()])-set(cities_visited[k]))\n",
        "\n",
        "#                     summation = 0.0\n",
        "#                     for city in cities_not_visited:\n",
        "#                         pheromone = self.graph.obtainPheromoneEdge(self.ants[k].obtainCity(),city)\n",
        "#                         distance = self.graph.obtainCostEdge(self.ants[k].obtainCity(),city)\n",
        "#                         summation += (math.pow(pheromone,self.alpha) * math.pow(1.0 / distance,self.beta))\n",
        "\n",
        "#                     probabilities = {}\n",
        "#                     for city in cities_not_visited:\n",
        "#                         pheromone = self.graph.obtainPheromoneEdge(self.ants[k].obtainCity(),city)\n",
        "#                         distance = self.graph.obtainCostEdge(self.ants[k].obtainCity(),city)\n",
        "#                         probability = (math.pow(pheromone,self.alpha) *\n",
        "#                                        math.pow(1.0 / distance,self.beta)) / (summation if summation > 0 else 1)\n",
        "#                         probabilities[city] = probability\n",
        "#                     city_selected = max(probabilities, key=probabilities.get)\n",
        "#                     cities_visited[k].append(city_selected)\n",
        "\n",
        "#             cities_visited_PD = pd.DataFrame(cities_visited)\n",
        "#             List_FS = cities_visited_PD.iloc[:, 0:self.num_FS].values\n",
        "\n",
        "#             X_train, X_test, y_train, y_test = train_test_split(data_bank, target,test_size=0.20,random_state=42)\n",
        "\n",
        "#             for x in range(self.num_ants):\n",
        "#                 clf = ExtraTreesClassifier()\n",
        "#                 clf.fit(X_train.iloc[:,List_FS[x]],y_train)\n",
        "\n",
        "#                 y_pred = clf.predict(X_test.iloc[:,List_FS[x]])\n",
        "#                 self.ants[x].setSolution(List_FS[x], accuracy_score(y_test,y_pred))\n",
        "\n",
        "\n",
        "#             best_solution = []\n",
        "#             best_acc = None\n",
        "#             for k in range(self.num_ants):\n",
        "#                 if not best_acc:\n",
        "#                     best_acc = self.ants[k].obtainAccuracy()\n",
        "#                 else:\n",
        "#                     aux_acc = self.ants[k].obtainAccuracy()\n",
        "#                     if aux_acc > best_acc:\n",
        "#                         best_solution = self.ants[k].obtainSolution()\n",
        "#                         best_acc = aux_acc\n",
        "\n",
        "#             self.accuracies.append(Accuracy(iteration = it))\n",
        "#             self.accuracies[it].setSolution(solution = best_solution,accuracy = best_acc)\n",
        "\n",
        "#             for edge in self.graph.edges:\n",
        "#                 sum_pheromone = 0.0\n",
        "#                 for k in range(self.num_ants):\n",
        "#                     edges_ant = []\n",
        "#                     for j in range(self.graph.num_vertices - 1):\n",
        "#                         edges_ant.append((cities_visited[k][j],cities_visited[k][j + 1]))\n",
        "#                     edges_ant.append((cities_visited[k][-1], cities_visited[k][0]))\n",
        "\n",
        "#                     if edge in edges_ant:\n",
        "#                         sum_pheromone += (1.0 / self.graph.obtainCostPath(cities_visited[k]))\n",
        "#                 new_pheromone = (1.0 - self.evaporation) * self.graph.obtainPheromoneEdge(edge[0], edge[1]) + sum_pheromone\n",
        "#                 self.graph.setPheromoneEdge(edge[0], edge[1], new_pheromone)\n",
        "\n",
        "#         solution_final = []\n",
        "#         acc_final = None\n",
        "#         for k in range(self.iterations):\n",
        "#             if not acc_final:\n",
        "#                 solution_final = self.accuracies[k].obtainSolution_final()[:]\n",
        "#                 acc_final = self.accuracies[k].obtainAccuracy_final()\n",
        "#                 print(acc_final)\n",
        "#             else:\n",
        "#                 aux_acc = self.accuracies[k].obtainAccuracy_final()\n",
        "#                 if aux_acc > acc_final:\n",
        "\n",
        "#                     solution_final = self.accuracies[k].obtainSolution_final()[:]\n",
        "#                     acc_final = self.accuracies[k].obtainAccuracy_final()\n",
        "#         acc_final=acc_final*100\n",
        "\n",
        "#         print('Solution(sub-set) of attributes that presented the highest accuracy over',self.iterations,'iterations:')\n",
        "#         print('%s ' % (' -> '.join(str(i) for i in solution_final)))\n",
        "#         print(\"Accuracy for Extra Trees: \",acc_final)\n",
        "\n",
        "\n",
        "# data = pd.read_csv(r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv')\n",
        "# # data_columns = data.drop(['target'],axis=1)\n",
        "# label = data['num'].values\n",
        "# from sklearn import preprocessing\n",
        "# # normalized_data = preprocessing.normalize(data_columns)\n",
        "# # for i in range(0,303):\n",
        "# #     data_columns.loc[i,('age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope',\n",
        "# #                         'ca','thal')]=normalized_data[i]\n",
        "\n",
        "\n",
        "# print('Dataset Information(Samples, Attributes):', data.shape)\n",
        "\n",
        "# def cosine_distance(v1, v2):\n",
        "#     v1_abs = np.abs(v1)\n",
        "#     v2_abs = np.abs(v2)\n",
        "#     # Compute the cosine similarity between two vectors\n",
        "#     similarity = 1 - spatial.distance.cosine(v1_abs, v2_abs)\n",
        "#     # Handle cases where the similarity is very close to 1 or -1\n",
        "#     if np.isclose(similarity, 1.0):\n",
        "#         return 0.0  # Return 0 distance for identical vectors\n",
        "#     elif np.isclose(similarity, -1.0):\n",
        "#         return np.inf  # Return infinite distance for opposite vectors\n",
        "#     # Convert similarity to distance\n",
        "#     distance = 1 - similarity\n",
        "#     return distance\n",
        "\n",
        "\n",
        "# matrix = np.zeros((data.shape[1], data.shape[1]))\n",
        "\n",
        "# for k in range(len(data.columns)):\n",
        "#     data_1 = data.iloc[:, k].values\n",
        "\n",
        "#     for j in range(len(data.columns)):\n",
        "#         data_2 = data.iloc[:, j].values\n",
        "#         matrix[k, j] = cosine_distance(data_1, data_2)\n",
        "#         j += 1\n",
        "#     k += 1\n",
        "\n",
        "# df_matrix_similarity = pd.DataFrame(matrix, columns=data.columns, index=data.columns)\n",
        "\n",
        "# num_vertices = 13\n",
        "\n",
        "# graph_complete = GraphComplete(num_vertices=num_vertices)\n",
        "# graph_complete.generate(matrix)\n",
        "\n",
        "# aco2 = ACO(graph=graph_complete, num_ants=graph_complete.num_vertices, alpha=1,\n",
        "#            beta=1, iterations=20,evaporation=0.2, num_FS=10)\n",
        "\n",
        "# aco2.print()\n",
        "\n",
        "# aco2.run(data_bank = data, target = label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d478b758",
      "metadata": {
        "id": "d478b758",
        "outputId": "cc933002-4aae-48c9-f494-79ebfb59d495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting spatialNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "  Downloading spatial-0.2.0.tar.gz (7.9 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Building wheels for collected packages: spatial\n",
            "  Building wheel for spatial (setup.py): started\n",
            "  Building wheel for spatial (setup.py): finished with status 'done'\n",
            "  Created wheel for spatial: filename=spatial-0.2.0-py3-none-any.whl size=8515 sha256=048a862f08856ca42fe4a96ff04747506a00a434ac90bcd14383dce3835b2b22\n",
            "  Stored in directory: c:\\users\\thasneem fathima\\appdata\\local\\pip\\cache\\wheels\\7f\\a3\\95\\331f7097309a3abde3983f5471678b37deca55aafa0ee0768f\n",
            "Successfully built spatial\n",
            "Installing collected packages: spatial\n",
            "Successfully installed spatial-0.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install spatial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62dc95a9",
      "metadata": {
        "id": "62dc95a9",
        "outputId": "1eeab310-cd5a-4422-8b54-acb39866e3c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9637681159420289\n",
            "Recall: 0.9637681159420289\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# # Load the dataset\n",
        "# file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\aco.csv'\n",
        "# data = pd.read_csv(file_path)\n",
        "# # data.drop(columns=['fbs'], inplace=True)\n",
        "# # Define the target column\n",
        "# target_column = \"num\"\n",
        "# # data.drop(columns=['age'], inplace=True)\n",
        "# # Split the dataset into features (X) and target variable (y)\n",
        "# X = data.drop(columns=[target_column])\n",
        "# y = data[target_column]\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Initialize the Extra Trees Classifier\n",
        "# et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# # Train the classifier\n",
        "# et_classifier.fit(X_train, y_train)\n",
        "\n",
        "# # Predict on the test set\n",
        "# y_pred = et_classifier.predict(X_test)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# # Calculate recall\n",
        "# # Choose one of the average options: 'micro', 'macro', or 'weighted'\n",
        "# recall = recall_score(y_test, y_pred, average='micro')\n",
        "# print(\"Recall:\", recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fec71143",
      "metadata": {
        "id": "fec71143",
        "outputId": "49ce9353-229b-4529-cb4d-99a50c6ead31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          age  sex        cp  trestbps      chol  restecg    thalch   oldpeak  \\\n",
            "0    0.489408    1  0.299439  0.316918  0.326599      0.0  0.749192  0.276246   \n",
            "1    0.545558    1  0.000000  0.380050  0.366284      0.0  0.299136  0.252751   \n",
            "2    0.615378    1  0.606874  0.197733  0.276693      0.0  0.694964  0.395993   \n",
            "3    0.135869    1  0.000000  0.252442  0.215091      0.5  0.442779  0.256535   \n",
            "4    0.505407    1  0.000000  0.589802  0.372818      0.0  0.519713  0.230092   \n",
            "..        ...  ...       ...       ...       ...      ...       ...       ...   \n",
            "683  0.644220    1  0.000000  0.346804  0.245977      0.5  0.673315  0.005793   \n",
            "684  0.297821    1  0.016412  0.519012  0.318826      0.5  0.763359  0.234788   \n",
            "685  0.562500    1  0.000000  0.245283  0.000000      0.5  0.374046  0.241935   \n",
            "686  0.637624    1  0.309789  0.301115  0.206336      0.0  0.554102  0.498201   \n",
            "687  0.676074    1  0.763435  0.428485  0.243879      0.0  0.683327  0.408953   \n",
            "\n",
            "        slope        ca      thal  \n",
            "0    0.724579  0.299439  1.000000  \n",
            "1    0.500000  0.333333  0.604411  \n",
            "2    0.500000  0.363230  0.910311  \n",
            "3    0.500000  0.007905  1.000000  \n",
            "4    0.500000  0.217301  1.000000  \n",
            "..        ...       ...       ...  \n",
            "683  1.000000  0.333333  1.000000  \n",
            "684  1.000000  0.000000  0.500000  \n",
            "685  0.500000  0.000000  1.000000  \n",
            "686  0.500000  0.333333  0.732342  \n",
            "687  0.500000  0.096769  0.500000  \n",
            "\n",
            "[688 rows x 11 columns]\n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.9420289855072463  \n",
            "0.927536231884058  \n",
            "0.9492753623188406  \n",
            "0.9347826086956522  \n",
            "0.9420289855072463  \n",
            "0.9492753623188406  \n",
            "0.9492753623188406  \n",
            "0.927536231884058  \n",
            "0.9565217391304348  \n",
            "0.927536231884058  \n",
            "0.927536231884058  \n",
            "0.9565217391304348  \n",
            "0.9420289855072463  \n",
            "0.927536231884058  \n",
            "0.9492753623188406  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9420289855072463  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "0.9420289855072463  \n",
            "0.9565217391304348  \n",
            "0.9565217391304348  \n",
            "FINAL:\n",
            "[99.4414946619204, 11.041829136947428, 2.3822911573511165, 1, 0.2858281204208786]\n",
            "0.9565217391304348\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.PSO at 0x1ad36149990>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.model_selection import cross_val_score\n",
        "# from sklearn.metrics import accuracy_score\n",
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# import numpy as np\n",
        "# import random\n",
        "\n",
        "# # Load the dataset\n",
        "# data = pd.read_csv(r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\aco.csv')\n",
        "\n",
        "# # Separate features and target variable\n",
        "# X = data.drop('num', axis=1)\n",
        "# y = data['num']\n",
        "# print(X)\n",
        "# # Split data into train and test sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "# class Particle:\n",
        "#     def __init__(self, params):\n",
        "#         self.params_i = []           # particle parameters\n",
        "#         self.velocity_i = []         # particle velocity\n",
        "#         self.pos_best_i = []         # best position individual\n",
        "#         self.err_best_i = -1         # best error individual\n",
        "#         self.err_i = -1              # error individual\n",
        "\n",
        "#         for param in params:\n",
        "#             self.velocity_i.append(random.uniform(-1, 1))\n",
        "#             self.params_i.append(param)\n",
        "\n",
        "#     # evaluate current fitness\n",
        "#     def evaluate(self, costFunc):\n",
        "#         self.err_i = costFunc(self.params_i)\n",
        "\n",
        "#         # check to see if the current position is an individual best\n",
        "#         if self.err_i >= self.err_best_i:\n",
        "#             print(self.err_i,\" \")\n",
        "#             self.pos_best_i = self.params_i\n",
        "#             self.err_best_i = self.err_i\n",
        "\n",
        "#     # update new particle velocity\n",
        "#     def update_velocity(self, pos_best_g):\n",
        "#         w = 0.5       # constant inertia weight (how much to weigh the previous velocity)\n",
        "#         c1 = 1        # cognitive constant\n",
        "#         c2 = 2        # social constant\n",
        "\n",
        "#         for i in range(len(self.params_i)):\n",
        "#             r1 = random.random()\n",
        "#             r2 = random.random()\n",
        "\n",
        "#             vel_cognitive = c1 * r1 * (self.pos_best_i[i] - self.params_i[i])\n",
        "#             vel_social = c2 * r2 * (pos_best_g[i] - self.params_i[i])\n",
        "#             self.velocity_i[i] = w * self.velocity_i[i] + vel_cognitive + vel_social\n",
        "\n",
        "#     # update the particle position based off new velocity updates\n",
        "#     def update_position(self, bounds):\n",
        "#         for i in range(len(self.params_i)):\n",
        "#             self.params_i[i] = self.params_i[i] + self.velocity_i[i]\n",
        "\n",
        "#             # adjust maximum position if necessary\n",
        "#             if self.params_i[i] > bounds[i][1]:\n",
        "#                 self.params_i[i] = bounds[i][1]\n",
        "\n",
        "#             # adjust minimum position if necessary\n",
        "#             if self.params_i[i] < bounds[i][0]:\n",
        "#                 self.params_i[i] = bounds[i][0]\n",
        "\n",
        "# class PSO:\n",
        "#     def __init__(self, costFunc, params, bounds, num_particles, maxiter):\n",
        "#         global num_dimensions\n",
        "\n",
        "#         num_dimensions = len(params)\n",
        "#         err_best_g = -1                   # best error for group\n",
        "#         pos_best_g = []                   # best position for group\n",
        "\n",
        "#         # establish the swarm\n",
        "#         swarm = []\n",
        "#         for i in range(num_particles):\n",
        "#             swarm.append(Particle(params))\n",
        "\n",
        "#         # begin optimization loop\n",
        "#         i = 0\n",
        "#         while i < maxiter:\n",
        "#             # cycle through particles in swarm and evaluate fitness\n",
        "#             for j in range(num_particles):\n",
        "#                 swarm[j].evaluate(costFunc)\n",
        "\n",
        "#                 # determine if current particle is the best (globally)\n",
        "#                 if swarm[j].err_i >= err_best_g:\n",
        "#                     pos_best_g = list(swarm[j].pos_best_i)\n",
        "#                     err_best_g = float(swarm[j].err_i)\n",
        "\n",
        "#             # cycle through swarm and update velocities and position\n",
        "#             for j in range(num_particles):\n",
        "#                 swarm[j].update_velocity(pos_best_g)\n",
        "#                 swarm[j].update_position(bounds)\n",
        "#             i += 1\n",
        "\n",
        "#         # print final results\n",
        "#         print('FINAL:')\n",
        "#         print(pos_best_g)\n",
        "#         print(err_best_g)\n",
        "\n",
        "# def evaluate_params(params):\n",
        "#     # Evaluate Extra Trees Classifier with given hyperparameters\n",
        "# #     print(params)\n",
        "#     extra_trees_clf = ExtraTreesClassifier(n_estimators=int(params[0]), max_depth=int(params[1]),\n",
        "#                                            min_samples_split=int(params[2]), min_samples_leaf=int(params[3]),\n",
        "#                                            max_features=params[4], random_state=42)\n",
        "#     extra_trees_clf.fit(X_train, y_train)\n",
        "\n",
        "#     # Predict on the testing data\n",
        "#     y_pred = extra_trees_clf.predict(X_test)\n",
        "\n",
        "#     # Calculate accuracy\n",
        "#     accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#     return accuracy\n",
        "\n",
        "# initial = [100, 10, 2, 1, 0.5]  # initial starting location [n_estimators, max_depth, min_samples_split, min_samples_leaf, max_features]\n",
        "# bounds = [(10, 500), (1, 50), (2, 20), (1, 10), (0.1, 0.9)]  # input bounds [(param_min, param_max), ...]\n",
        "# PSO(evaluate_params, initial, bounds, num_particles=15, maxiter=5)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e4082a",
      "metadata": {
        "id": "a6e4082a",
        "outputId": "a2bf37ee-9e50-4740-8f0d-64a004ec40c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9782608695652174\n",
            "Recall: 0.9782608695652174\n"
          ]
        }
      ],
      "source": [
        "# import pandas as pd\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.metrics import accuracy_score, recall_score\n",
        "\n",
        "# # Load the dataset\n",
        "# # file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smoteagain.csv'\n",
        "# file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv'\n",
        "# data = pd.read_csv(file_path)\n",
        "# data.drop(columns=['trestbps','restecg','exang'], inplace=True)\n",
        "\n",
        "# target_column = \"num\"\n",
        "# # data.drop(columns=['age'], inplace=True)\n",
        "# # Split the dataset into features (X) and target variable (y)\n",
        "# X = data.drop(columns=[target_column])\n",
        "# # feature_columns = ['sex', 'cp', 'chol', 'thalch', 'exang', 'slope']\n",
        "# # X = data[feature_columns]\n",
        "\n",
        "# y = data[target_column]\n",
        "\n",
        "# # Split the data into training and testing sets\n",
        "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# # Initialize the Extra Trees Classifier\n",
        "# et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# # Train the classifier\n",
        "# et_classifier.fit(X_train, y_train)\n",
        "\n",
        "# # Predict on the test set\n",
        "# y_pred = et_classifier.predict(X_test)\n",
        "\n",
        "# # Calculate accuracy\n",
        "# accuracy = accuracy_score(y_test, y_pred)\n",
        "# print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# # Calculate recall\n",
        "# # Choose one of the average options: 'micro', 'macro', or 'weighted'\n",
        "# recall = recall_score(y_test, y_pred, average='micro')\n",
        "# print(\"Recall:\", recall)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c60cf3",
      "metadata": {
        "scrolled": true,
        "id": "25c60cf3",
        "outputId": "772f32b2-0e1f-44e2-9165-d5c2beeee2a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: ['Best', [1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1],           age        cp  trestbps      chol  fbs    thalch   oldpeak  \\\n",
            "0    0.489408  0.299439  0.316918  0.326599    0  0.749192  0.276246   \n",
            "1    0.545558  0.000000  0.380050  0.366284    0  0.299136  0.252751   \n",
            "2    0.615378  0.606874  0.197733  0.276693    0  0.694964  0.395993   \n",
            "3    0.135869  0.000000  0.252442  0.215091    0  0.442779  0.256535   \n",
            "4    0.505407  0.000000  0.589802  0.372818    0  0.519713  0.230092   \n",
            "..        ...       ...       ...       ...  ...       ...       ...   \n",
            "683  0.644220  0.000000  0.346804  0.245977    0  0.673315  0.005793   \n",
            "684  0.297821  0.016412  0.519012  0.318826    0  0.763359  0.234788   \n",
            "685  0.562500  0.000000  0.245283  0.000000    0  0.374046  0.241935   \n",
            "686  0.637624  0.309789  0.301115  0.206336    0  0.554102  0.498201   \n",
            "687  0.676074  0.763435  0.428485  0.243879    0  0.683327  0.408953   \n",
            "\n",
            "           ca      thal  num  \n",
            "0    0.299439  1.000000    3  \n",
            "1    0.333333  0.604411    1  \n",
            "2    0.363230  0.910311    4  \n",
            "3    0.007905  1.000000    1  \n",
            "4    0.217301  1.000000    4  \n",
            "..        ...       ...  ...  \n",
            "683  0.333333  1.000000    2  \n",
            "684  0.000000  0.500000    0  \n",
            "685  0.000000  1.000000    1  \n",
            "686  0.333333  0.732342    4  \n",
            "687  0.096769  0.500000    1  \n",
            "\n",
            "[688 rows x 10 columns], 0.9782608695652174]\n"
          ]
        }
      ],
      "source": [
        "# import pandas\n",
        "# import random\n",
        "# import numpy as np\n",
        "# import copy\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.ensemble import ExtraTreesClassifier\n",
        "# from sklearn.metrics import accuracy_score\n",
        "\n",
        "# class ACO:\n",
        "#     def __init__(self, data, maxIteration, antNumber, cc, Q, e):\n",
        "#         self.data = data\n",
        "#         self.fp = [cc] * (len(data.columns) - 1)\n",
        "#         self.maxIteration = maxIteration\n",
        "#         self.ants = []\n",
        "#         self.size = len(data.columns) - 1\n",
        "#         self.antNumber = antNumber\n",
        "#         self.Q = Q\n",
        "#         self.bestScore = 0\n",
        "#         self.result = []\n",
        "#         self.evaporate = e\n",
        "#         self.colonyMax = 0\n",
        "#         self.colonyQuality = 0\n",
        "\n",
        "#     def constructSolution(self, ant):\n",
        "#         featureSetIndex = []\n",
        "#         for j in range(self.size):\n",
        "#             decision = random.random()\n",
        "#             if decision < self.fp[j] / 2.0:\n",
        "#                 featureSetIndex.append(1)\n",
        "#             else:\n",
        "#                 featureSetIndex.append(0)\n",
        "# #         print(featureSetIndex)\n",
        "# #         featureSetIndex.append(1)\n",
        "#         features = []\n",
        "#         for i, obj in enumerate(featureSetIndex):\n",
        "#             if obj:\n",
        "#                 features.append(i)\n",
        "\n",
        "#         features.append(13)\n",
        "# #         print(features)\n",
        "#         newdata = self.data.iloc[:, features]\n",
        "# #         print(newdata.columns)\n",
        "#         if sum(featureSetIndex) == 0:\n",
        "#             score = 0.5\n",
        "#         else:\n",
        "# #             X_train, X_test, y_train, y_test = train_test_split(newdata, self.data['num'], test_size=0.2, random_state=42)\n",
        "#             X = newdata.drop(columns=['num'])\n",
        "#             y= newdata['num']\n",
        "#             X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#             clf = ExtraTreesClassifier(random_state=42)\n",
        "#             clf.fit(X_train, y_train)\n",
        "#             y_pred = clf.predict(X_test)\n",
        "#             score = accuracy_score(y_test, y_pred)\n",
        "# #             print(newdata.columns,score)\n",
        "#         ant.val = score\n",
        "#         ant.subsets = copy.deepcopy(featureSetIndex)\n",
        "#         ant.dataset = newdata\n",
        "#         return ant\n",
        "\n",
        "\n",
        "#     def ApplyLocalSearch(self):\n",
        "#         maxScore = 0\n",
        "#         maxSet = []\n",
        "#         maxdataset = []\n",
        "#         for a in self.ants:\n",
        "#             if maxScore < a.val or (maxScore == a.val and (maxSet and sum(a.subsets) < sum(maxSet))):\n",
        "#                 maxScore = a.val\n",
        "#                 maxSet = a.subsets\n",
        "#                 maxdataset = a.dataset\n",
        "\n",
        "#         if self.bestScore <= maxScore or (maxScore == self.bestScore and (self.result and sum(maxSet) < sum(self.result))):\n",
        "#             self.bestScore = maxScore\n",
        "#             self.result = maxSet\n",
        "\n",
        "#         #print(maxScore)\n",
        "#         self.colonyMax += maxScore\n",
        "# #         print('i',maxSet,maxScore)\n",
        "#         return maxSet, maxScore, maxdataset\n",
        "\n",
        "#     def UpdatePheromones(self,bestSet, bestScore):\n",
        "#         for i,v in enumerate(bestSet):\n",
        "#             self.fp[i] = self.fp[i]*self.evaporate\n",
        "#             if v == 1:\n",
        "#                 weight = (bestScore-0.5)*2\n",
        "#                 self.fp[i] = self.fp[i] + self.Q*weight\n",
        "\n",
        "#     def simulate(self):\n",
        "#         for s in range(self.maxIteration):\n",
        "#             for i in range(self.antNumber):\n",
        "#                 ant = Ant()\n",
        "#                 ant = self.constructSolution(ant)\n",
        "#                 self.ants.append(ant)\n",
        "#             bestSet, bestScore, dataset = self.ApplyLocalSearch()\n",
        "#             self.UpdatePheromones(bestSet, bestScore)\n",
        "#             self.ants = []\n",
        "\n",
        "#         shortFeaturesName = list(self.data.columns.values)\n",
        "#         bestFeatureName = []\n",
        "#         for ind, obj in enumerate(self.result):\n",
        "#             if obj:\n",
        "#                 bestFeatureName.append(shortFeaturesName[ind + 1])\n",
        "#         new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\aco.csv'\n",
        "#         dataset.to_csv(new_file_path, index=False)\n",
        "#         return [\"Best\", bestSet, dataset, self.bestScore]\n",
        "\n",
        "#     def run(self):\n",
        "#         for s in range(self.maxIteration):\n",
        "#             for i in range(self.antNumber):\n",
        "#                 ant = Ant()\n",
        "#                 ant = self.constructSolution(ant)\n",
        "#                 self.ants.append(ant)\n",
        "#             bestSet, bestScore = self.ApplyLocalSearch()\n",
        "#             self.UpdatePheromones(bestSet, bestScore)\n",
        "#             self.ants = []\n",
        "\n",
        "#         shortFeaturesName = list(self.data.columns.values)\n",
        "#         bestFeatureName = []\n",
        "#         for ind, obj in enumerate(self.result):\n",
        "#             if obj:\n",
        "#                 bestFeatureName.append(shortFeaturesName[ind + 1])\n",
        "\n",
        "#         self.colonyQuality = self.colonyMax / self.maxIteration # the overall ant colony quality over iterations\n",
        "#         return self.bestScore, self.result, self.colonyQuality\n",
        "\n",
        "# class Ant:\n",
        "#     def __init__(self):\n",
        "#         self.subsets = []\n",
        "#         self.val = 0\n",
        "#         self.dataset = []\n",
        "\n",
        "# data = pd.read_csv(r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\smoteagain.csv')\n",
        "# acoModel = ACO(data,maxIteration=30,antNumber=30,cc=1,Q=0.1,e=0.95)\n",
        "# result = acoModel.simulate()\n",
        "# print('Result:', result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c3b574c",
      "metadata": {
        "id": "4c3b574c",
        "outputId": "f71b30e5-0ae4-42d2-b4ac-6edf8e764e6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best subset of features: ['age', 'cp', 'trestbps', 'chol', 'restecg', 'thalch', 'oldpeak', 'slope', 'ca', 'thal']\n",
            "Accuracy: 0.9855072463768116\n"
          ]
        }
      ],
      "source": [
        "#5\n",
        "import pandas as pd\n",
        "from itertools import combinations\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Read the CSV file\n",
        "data = pd.read_csv(r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv') # Replace \"your_file.csv\" with the path to your CSV file\n",
        "\n",
        "# Separate features and target\n",
        "X = data.drop(columns=['num'])  # Assuming 'target' is the name of the target column\n",
        "y = data['num']\n",
        "\n",
        "# Initialize variables to keep track of the best subset and its accuracy\n",
        "best_subset = None\n",
        "best_accuracy = 0.0\n",
        "\n",
        "# Get all possible combinations of feature subsets\n",
        "feature_combinations = combinations(X.columns, 10)\n",
        "for subset in feature_combinations:\n",
        "    # Convert subset to list\n",
        "    subset_list = list(subset)\n",
        "\n",
        "    # Train-test split\n",
        "    X_subset = X[subset_list]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_subset, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    clf = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    # Predict and calculate accuracy\n",
        "    y_pred = clf.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    # Check if current subset has higher accuracy\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_subset = subset_list\n",
        "# Print the best subset and its accuracy\n",
        "print(\"Best subset of features:\", best_subset)\n",
        "print(\"Accuracy:\", best_accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0713a0d1",
      "metadata": {
        "id": "0713a0d1",
        "outputId": "e5f31297-aca4-453b-ec34-1147e07a8e89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Result: ['Best', [1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1],           age        cp  trestbps      chol  restecg    thalch   oldpeak  \\\n",
            "0    0.489408  0.299439  0.316918  0.326599      0.0  0.749192  0.276246   \n",
            "1    0.545558  0.000000  0.380050  0.366284      0.0  0.299136  0.252751   \n",
            "2    0.615378  0.606874  0.197733  0.276693      0.0  0.694964  0.395993   \n",
            "3    0.135869  0.000000  0.252442  0.215091      0.5  0.442779  0.256535   \n",
            "4    0.505407  0.000000  0.589802  0.372818      0.0  0.519713  0.230092   \n",
            "..        ...       ...       ...       ...      ...       ...       ...   \n",
            "683  0.644220  0.000000  0.346804  0.245977      0.5  0.673315  0.005793   \n",
            "684  0.297821  0.016412  0.519012  0.318826      0.5  0.763359  0.234788   \n",
            "685  0.562500  0.000000  0.245283  0.000000      0.5  0.374046  0.241935   \n",
            "686  0.637624  0.309789  0.301115  0.206336      0.0  0.554102  0.498201   \n",
            "687  0.676074  0.763435  0.428485  0.243879      0.0  0.683327  0.408953   \n",
            "\n",
            "        slope        ca      thal  num  \n",
            "0    0.724579  0.299439  1.000000    3  \n",
            "1    0.500000  0.333333  0.604411    1  \n",
            "2    0.500000  0.363230  0.910311    4  \n",
            "3    0.500000  0.007905  1.000000    1  \n",
            "4    0.500000  0.217301  1.000000    4  \n",
            "..        ...       ...       ...  ...  \n",
            "683  1.000000  0.333333  1.000000    2  \n",
            "684  1.000000  0.000000  0.500000    0  \n",
            "685  0.500000  0.000000  1.000000    1  \n",
            "686  0.500000  0.333333  0.732342    4  \n",
            "687  0.500000  0.096769  0.500000    1  \n",
            "\n",
            "[688 rows x 11 columns], 0.9855072463768116]\n"
          ]
        }
      ],
      "source": [
        "#6\n",
        "import pandas\n",
        "import random\n",
        "import numpy as np\n",
        "import copy\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class ACO:\n",
        "    def __init__(self, data, maxIteration, antNumber, cc, Q, e):\n",
        "        self.data = data\n",
        "        self.fp = [cc] * (len(data.columns) - 1)\n",
        "        self.maxIteration = maxIteration\n",
        "        self.ants = []\n",
        "        self.size = len(data.columns) - 1\n",
        "        self.antNumber = antNumber\n",
        "        self.Q = Q\n",
        "        self.bestScore = 0\n",
        "        self.result = []\n",
        "        self.evaporate = e\n",
        "        self.colonyMax = 0\n",
        "        self.colonyQuality = 0\n",
        "\n",
        "    def constructSolution(self, ant):\n",
        "        featureSetIndex = []\n",
        "        for j in range(self.size):\n",
        "            decision = random.random()\n",
        "            if decision < self.fp[j] / 2.0:\n",
        "                featureSetIndex.append(1)\n",
        "            else:\n",
        "                featureSetIndex.append(0)\n",
        "#         print(featureSetIndex)\n",
        "#         featureSetIndex.append(1)\n",
        "        features = []\n",
        "        for i, obj in enumerate(featureSetIndex):\n",
        "            if obj:\n",
        "                features.append(i)\n",
        "\n",
        "        features.append(12)\n",
        "#         print(features)\n",
        "        newdata = self.data.iloc[:, features]\n",
        "#         print(newdata.columns)\n",
        "        if sum(featureSetIndex) == 0:\n",
        "            score = 0.5\n",
        "        else:\n",
        "#             X_train, X_test, y_train, y_test = train_test_split(newdata, self.data['num'], test_size=0.2, random_state=42)\n",
        "            X = newdata.drop(columns=['num'])\n",
        "            y= newdata['num']\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "            clf = ExtraTreesClassifier(random_state=42)\n",
        "            clf.fit(X_train, y_train)\n",
        "            y_pred = clf.predict(X_test)\n",
        "            score = accuracy_score(y_test, y_pred)\n",
        "#             print(newdata.columns,score)\n",
        "        ant.val = score\n",
        "        ant.subsets = copy.deepcopy(featureSetIndex)\n",
        "        ant.dataset = newdata\n",
        "        return ant\n",
        "\n",
        "\n",
        "    def ApplyLocalSearch(self):\n",
        "        maxScore = 0\n",
        "        maxSet = []\n",
        "        maxdataset = []\n",
        "        for a in self.ants:\n",
        "            if maxScore < a.val or (maxScore == a.val and (maxSet and sum(a.subsets) < sum(maxSet))):\n",
        "                maxScore = a.val\n",
        "                maxSet = a.subsets\n",
        "                maxdataset = a.dataset\n",
        "\n",
        "        if self.bestScore <= maxScore or (maxScore == self.bestScore and (self.result and sum(maxSet) < sum(self.result))):\n",
        "            self.bestScore = maxScore\n",
        "            self.result = maxSet\n",
        "\n",
        "        #print(maxScore)\n",
        "        self.colonyMax += maxScore\n",
        "#         print('i',maxSet,maxScore)\n",
        "        return maxSet, maxScore, maxdataset\n",
        "\n",
        "    def UpdatePheromones(self,bestSet, bestScore):\n",
        "        for i,v in enumerate(bestSet):\n",
        "            self.fp[i] = self.fp[i]*self.evaporate\n",
        "            if v == 1:\n",
        "                weight = (bestScore-0.5)*2\n",
        "                self.fp[i] = self.fp[i] + self.Q*weight\n",
        "\n",
        "    def simulate(self):\n",
        "        for s in range(self.maxIteration):\n",
        "            for i in range(self.antNumber):\n",
        "                ant = Ant()\n",
        "                ant = self.constructSolution(ant)\n",
        "                self.ants.append(ant)\n",
        "            bestSet, bestScore, dataset = self.ApplyLocalSearch()\n",
        "            self.UpdatePheromones(bestSet, bestScore)\n",
        "            self.ants = []\n",
        "\n",
        "        shortFeaturesName = list(self.data.columns.values)\n",
        "        bestFeatureName = []\n",
        "        for ind, obj in enumerate(self.result):\n",
        "            if obj:\n",
        "                bestFeatureName.append(shortFeaturesName[ind + 1])\n",
        "        new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\aco2.csv'\n",
        "        dataset.to_csv(new_file_path, index=False)\n",
        "        return [\"Best\", bestSet, dataset, self.bestScore]\n",
        "\n",
        "    def run(self):\n",
        "        for s in range(self.maxIteration):\n",
        "            for i in range(self.antNumber):\n",
        "                ant = Ant()\n",
        "                ant = self.constructSolution(ant)\n",
        "                self.ants.append(ant)\n",
        "            bestSet, bestScore = self.ApplyLocalSearch()\n",
        "            self.UpdatePheromones(bestSet, bestScore)\n",
        "            self.ants = []\n",
        "\n",
        "        shortFeaturesName = list(self.data.columns.values)\n",
        "        bestFeatureName = []\n",
        "        for ind, obj in enumerate(self.result):\n",
        "            if obj:\n",
        "                bestFeatureName.append(shortFeaturesName[ind + 1])\n",
        "\n",
        "        self.colonyQuality = self.colonyMax / self.maxIteration # the overall ant colony quality over iterations\n",
        "        return self.bestScore, self.result, self.colonyQuality\n",
        "\n",
        "class Ant:\n",
        "    def __init__(self):\n",
        "        self.subsets = []\n",
        "        self.val = 0\n",
        "        self.dataset = []\n",
        "\n",
        "data = pd.read_csv(r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\fcbfdata.csv')\n",
        "acoModel = ACO(data,maxIteration=30,antNumber=30,cc=1,Q=0.1,e=0.95)\n",
        "result = acoModel.simulate()\n",
        "print('Result:', result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35dc60c3",
      "metadata": {
        "id": "35dc60c3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae76eb1",
      "metadata": {
        "id": "6ae76eb1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2642031",
      "metadata": {
        "id": "f2642031"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdea9bd5",
      "metadata": {
        "id": "cdea9bd5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b312751",
      "metadata": {
        "id": "2b312751"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3f0c4f4",
      "metadata": {
        "id": "f3f0c4f4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44a78030",
      "metadata": {
        "id": "44a78030"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc539e81",
      "metadata": {
        "id": "bc539e81"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}